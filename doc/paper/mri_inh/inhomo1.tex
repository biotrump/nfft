%\documentclass[11pt,a4paper,bibtotoc]{scrartcl}
\documentclass[journal]{IEEEtran}
%\documentclass[11pt,a4paper,twoside]{article}
%\usepackage[T1]{fontenc}
%\usepackage[latin1]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}%\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphics}
%\usepackage{showkeys}
\usepackage{times}
\usepackage{alltt}
\usepackage{latexsym}
%\usepackage{pstricks}
\usepackage{graphicx}
\usepackage{cite}
\def\ti{\mbox{\scriptsize{\rm i}}}
%\newcommand{\eipp}[1]{{\rm e}^{ \pi{\ti} #1}}
%\newcommand{\eimp}[1]{{\rm e}^{- \pi{\ti} #1}}
\newcommand{\eip}[1]{{\rm e}^{ 2\pi{\ti} #1}}
\newcommand{\eim}[1]{{\rm e}^{-2\pi{\ti} #1}}
\newcommand{\zb}[1]{\mbox{\boldmath{${#1}$}}}
\newcommand{\zbs}[1]{\mbox{\boldmath\scriptsize{${#1}$}}}
\newcommand{\zbss}[1]{\mbox{\boldmath\tiny{${#1}$}}}
\newcommand{\dist}{{\rm dist}}
\newcommand{\cond}{{\rm cond}}
\newcommand{\diag}{{\rm diag}}
\renewcommand{\mod}{\;{\rm mod}\;}
\renewcommand{\d}{{\rm d}}
\newcommand{\supp}{{\rm supp}}
\newcommand{\card}{{\rm card}}
\newcommand{\adj}{{\vdash \hspace*{-1.72mm} \dashv}}

\renewcommand{\Box}{\hspace*{0ex} \hfill \rule{1.5ex}{1.5ex} \\ \goodbreak}
\newcommand{\Boxgl}{\par\vspace{-5ex} \hspace*{0ex} \hfill
  \rule{1.5ex}{1.5ex} \\ \goodbreak\goodbreak}
\newcommand{\bend}{\hspace*{0ex} \hfill \hbox{\vrule height
    1.5ex\vbox{\hrule width 1.4ex \vskip 1.4ex\hrule  width 1.4ex}\vrule
    height 1.5ex} \goodbreak}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{numExp}{\bf Numerical Example}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{algorithm}[theorem]{Algorithm}

\newenvironment{Theorem}{\goodbreak \begin{theorem}\sl}{\end{theorem}}
\newenvironment{Lemma}{\goodbreak \begin{lemma}\sl}{\end{lemma}}
\newenvironment{Remark}{\goodbreak \begin{remark}\rm}{\hfill \bend\end{remark}}
\newenvironment{Example}{\goodbreak \begin{example}\rm}{\hfill \bend \end{example}}
\newenvironment{NumExp}{\goodbreak \begin{numExp}\rm}{\hfill \bend \end{numExp}}
\newenvironment{Definition}{\goodbreak \begin{definition}\rm}{\end{definition}}
\newenvironment{Corollary}{\goodbreak \begin{corollary}\rm}{\end{corollary}}
\newenvironment{Algorithm}{\begin{algorithm}\sl}{\hfill
  \end{algorithm}}

\renewcommand{\labelenumi}{\roman{enumi})} %aendert die Label
%\renewcommand{\theequation}{\arabic{section}.\arabic{equation}}

\numberwithin{equation}{section}
\numberwithin{table}{section}
\numberwithin{figure}{section}

\setcounter{totalnumber}{10}
\setcounter{topnumber}{10}

\title{Image reconstruction for MRI in the Presence of
  Filed Inhomogeneties}

\author{
Holger Eggers\thanks{\tt Philips},
Tobias Knopp\thanks{University of L\"ubeck,
Institute of Mathematics,
D--23560 L\"ubeck,
Germany, {\tt tobias.knopp@informatik.uni-luebeck.de}}, 
Daniel Potts\thanks{University of L\"ubeck,
Institute of Mathematics,
D--23560 L\"ubeck,
Germany, {\tt potts@math.uni-luebeck.de}}
}

\begin{document}

%\date{}

\maketitle

%\markboth{Journal of \LaTeX\ Class Files,~Vol.~1,
%No.~11,~November~2002}{Shell \MakeLowercase{\textit{et al.}}: Bare
%Demo of IEEEtran.cls for Journals} 

\begin{abstract}

\end{abstract}

2000 {\it Mathematics Subject Classification} 65T50, 68U10

%{\it Key words and phrases:} 
\begin{keywords}
2d/3d image reconstruction, iterative methods, magnetic resonance imaging, Fourier transform, FFT, NFFT, gridding, rose scan, spiral scan.
\end{keywords}


%------------------------------------------------------------------------------
\section{Introduction}
% Hier sollte motivert werden, warum die Inhomogentitäten wieder interessant werden: kürzere Röhren? usw.
\section{Theory}\label{Sec:Th}

In MRI, ignoring relaxation effects, the relation between the MR signal
$s_{\text{MR}}$ during the readout and the object $\tilde p$ can be modelled by
\begin{equation}\label{signal_eq}
s_{\text{MR}}\left(t\right) =
\int_{\mathbb{R}^3} \tilde p \left(\zb r\right) c\left(\zb r\right)
{\rm e}^{{\rm\scriptsize i}\omega(\zbs r)(t+T_E)}
{\rm e}^{2\pi {\rm\scriptsize i}\zbs r \zbs k\left(t\right)} \d \zb r\, ,
\end{equation}
where $T_E$ is the echo time,  
$c\left(\zb r\right)$ is the known sensitivity of the receiver coil,
$\omega(\zb r)$ is the field inhomogeneity present at $\zb r$, 
and
$\zb k(t)$ is the k-space trajectory.
%see \cite{SuNoFe03} for a low rank approximation of these effects.
We follow \cite{SuNoFe03}  and let for simplicity
$$
p(\zb r)= \tilde p(\zb r) c(\zb r) 
{\rm e}^{{\rm\scriptsize i}\omega(\zbs r)T_E}\, .
$$
Accurate estimation of $p(\zb r)$ yield $\tilde p(\zb r)$ assuming the
sensitivity and field maps are known.
For convenience let the available samples in k-space be contained in the shifted unit square, i.e.
$\zb k \in [-\frac{1}{2},\frac{1}{2})^2$, and the field of view be restricted to 
$\Omega_{\zbs N} \subset [-\frac{N_1}{2},\frac{N_1}{2})\times[-\frac{N_2}{2},\frac{N_2}{2})$,
where $\zb N=(N_1,N_2)^{\top} \in 2\mathbb{N}^2$.
Then, the discretisation of integral \eqref{signal_eq} on equispaced points leads to
\begin{equation}\label{inverse2}
s(\zb k(t)) \approx \tilde{s}(\zb k(t)) := \sum_{\zbs r \in I^2_{\zbss
    N}} p(\zb r) 
{\rm e}^{{\rm\scriptsize i}\omega(\zbs r)t}
{\rm e}^{2\pi {\rm\scriptsize i}\zbs r \zbs k(t)}
\, ,
\end{equation}
where $I^2_{\zbs N} :=
\{-\frac{N_1}{2},\dots,\frac{N_1}{2}-1\}\times\{-\frac{N_2}{2},\dots,
\frac{N_2}{2}-1\}$.

We denote the vector of the given values by $\zb {s}:=(\tilde s({\zb k_j}))_{j=0,\ldots,M-1}\in \mathbb{C}^{M}$,
the reshaped vector of the unknown object by $\zb {p}:=(p({\zb r}))_{\zbs r\in I_{\zbss N}^2} \in
\mathbb{C}^{N_1\times N_2}$ and the matrix 
\begin{equation}\label{MatrixB}
\zb H:=\left({\rm e}^{{\rm\scriptsize i}\omega(\zbs r)t_j} \eip{\zbs r\zbs k(t_j)}\right)_{j=0,\hdots,M-1;\;\zbs r\in
  I_{\zbss N}^2}\, ,
\end{equation}
Thus, the unknown object $p$ is given {\em implicit} by \eqref{inverse2}.
The authors of \cite{DeWaLe02} call this inverse model.
We propose to solve this system of linear equations by weighted least
squares, where the 
weights $w_j> 0$ compensate for clusters in the sampling set, i.e.,
\begin{equation} \label{eq:min}
  \|\zb s - \zb H \zb  p\|_{\zbs W}
  = \left(\sum_{j=0}^{M-1} w_j |s(\zb k_j)-\tilde s(\zb k_j)|^2\right)^{1/2}
  \stackrel{\zbs p}{\rightarrow} \min,
\end{equation}
where $\zb W:=\diag(w_j)_{j=0,\ldots,M-1}$.
This problem is equivalent to the weighted normal equation of first kind
\begin{equation}
  \label{eq:no1}
  \zb H^{\adj} \zb W \zb H \zb p = \zb H^{\adj} \zb W \zb s.
\end{equation}
The iterative solution of problem \eqref{eq:no1} can be solved 
by a suitable variant of conjugated gradients (CGNR). 

We state problem \eqref{inverse2} as the weighted approximation problem
\begin{equation}\label{minMR}
  \|\zb s - \zb {\tilde s}\|_{\zbs W}
  \stackrel{\zbs {p}}{\rightarrow} \min,
\end{equation}
where $\zb {\tilde s}=\zb H\zb p$, see \eqref{inverse2}, and the density compensation weights are collected in the diagonal matrix
$\zb W=\diag(w_j)_{j=0,\ldots,M-1}$. 
We would like to emphasise that the CGNR method minimises \eqref{minMR} in each iteration over a certain Krylov space.
%Thus, iterate for only one step, we obtain the 'optimised' gridding
%solution.
An algorithm for the fast multiplication with the matrix $\zb H$ and
$\zb H^{\top}$  is the key for CGNR and so for a fast solution of \eqref{inverse2}.

\section{NFFT}\label{Sec:NFFT}

In the following we briefly describe the NFFT, which has, for a fixed accuracy,
the same arithmetical complexity as the FFT. One can use the
software package, which is available from the NFFT homepage \cite{kupo02C}.

The NFFT can be introduced as follows. 
Let $\varphi\in L^2(\mathbb{R})\cap L^1(\mathbb{R}) $ given, such that
the one periodisation
\begin{equation} \label{phiper}
\tilde\varphi(x) :=  \sum_{r=-\infty}^{\infty} \varphi(x+r)
\end{equation} 
has a uniformly convergent Fourier series.
Hence we write the function $\tilde\varphi$ as Fourier series 
\begin{equation} \label{FRphi}
\tilde\varphi (x) =  \sum_{ k=-\infty }^{\infty}  c_{k}(\tilde\varphi)  
\eip{ k x} 
\end{equation} 
with the Fourier coefficients 
\begin{equation} \label{FKphi}
c_{ k} (\tilde \varphi ) :=  
\int\limits_{-1/2}^{1/2} \tilde\varphi ( x)
  \eim{ kx} \, {\rm d} {x} \quad ( k \in \mathbb{Z}) 
\end{equation}
and we obtain 
\begin{equation}\label{FK=ck}
\hat\varphi(k) := \int\limits_{-\infty}^{\infty} \varphi ( x)
  \eim{ k x} \, \d { x} = c_{ k} (\tilde \varphi )\, .
\end{equation}
The Fourier coefficients
\begin{equation*}
c_{ k} (\tilde \varphi ) =  
\int\limits_{-\infty}^{\infty} \tilde\varphi (x-y)
  \eim{ k(x-y)} \, {\rm d} {y} \quad ( k \in \mathbb{Z}, x\in[-1/2,1/2)) 
\end{equation*}
can be approximated for $k=-N/2,\ldots, N/2-1$ by
\begin{equation} \label{approx1}
c_{ k} (\tilde \varphi ) \approx
\frac{1}{\alpha N} \sum_{l=-\alpha N/2}^{\alpha N/2-1}
\tilde\varphi (x-\frac{l}{\alpha N})  \eim{ k(x-\frac{l}{\alpha N})} \,
\end{equation}
with an oversampling factor $\alpha >1$.
But \eqref{approx1} can be written as
\[
c_{ k} (\tilde \varphi ) \approx
\frac{1}{\alpha N}\eim{kx}
\sum_{l=-\alpha N/2}^{\alpha N/2-1}
\tilde\varphi (x-\frac{l}{\alpha N})  \eim{\frac{kl}{\alpha N}} \,,\quad
\]
and further for $k=-N/2,\ldots,N/2$
\begin{equation} \label{approx}
\eip{kx}
\approx
\frac{1}{nc_{ k} (\tilde \varphi )}
\sum_{l=-n/2}^{n/2-1}
\tilde\psi (x-\frac{l}{\alpha N})  \eim{\frac{kl}{\alpha N}}
\end{equation}
where $\tilde\psi$ is the one periodisation 
$
\displaystyle\tilde\psi(x) := \sum_{r=-\infty}^{\infty} \psi(x +r)\,
\in L^2(\mathbb{T})
$
and $\psi$ the truncation of $\varphi$, i.e.
\begin{equation}\label{Defpsi}
\psi(x) := \left\{
\begin{array}{ll}
\varphi(x) & \textrm{if }  x \in [- m/n,m/n], \\
0 & \textrm{else},
\end{array} \right.
\end{equation}
with a truncation parameter $m$ $(m\ll N,m\in \mathbb{N})$. 
Note that the approximation of the exponentials in \eqref{approx} is
used in order to develop the NFFT. The methods differ only by choosing
different window functions $\varphi$. 

For a finite number of  
given Fourier coefficients $\hat f_{k} \in \mathbb{C}$
we want to
evaluate the trigonometric polynomial  
\begin{equation}
 \label{trigPoly}
  f\left( x\right) := \sum_{k=-N/2}^{N/2-1} \hat{f}_{k} \eip{ k x}
\end{equation}
at given non equispaced knots $ x_j \in
[-\frac{1}{2},\frac{1}{2})^d,\;j=0,\hdots,M-1$. 
Thus, our concern is the fast evaluation of
\begin{equation*}
 f_j :=f(x_j) = \sum_{k=-N/2}^{N/2-1} \hat{f}_{k} \eip{k x_j}\,, \qquad
 j=0,\hdots,M-1. 
\end{equation*}
Using now the approximation of the exponentials given in \eqref{approx}
we obtain
\begin{equation}
 \label{NFFTapr}
f_j \approx\sum_{l=-\alpha N/2}^{\alpha N/2-1}\left(
\sum_{ k=-N/2}^{N/2-1} \frac{\hat{f}_{k}}{\alpha N c_{k} (\tilde \varphi )}
\eim{\frac{kl}{\alpha N}}
\right)\tilde\psi (x_j-\frac{l}{\alpha N})  \, .
\end{equation}
In matrix vector notation \eqref{trigPoly} reads as
\begin{equation}
 \label{ndft}
 \zb f=\zb A \zb {\hat f}
\end{equation}
where 
\begin{equation*}
\zb A:=\left(\eim{k x_j}\right)_{j=0,\hdots,M-1;\;k=-N/2,\ldots,N/2-1}\, ,
\end{equation*}
\begin{equation*}
\zb f:=\left(f_j\right)_{j=0,\hdots,M-1},\quad 
\zb {\hat f}:=\left(\hat{f}_{\zbs k}\right)_{k=-N/2,\ldots,N/2-1}.
\end{equation*}
and the approximation \eqref{NFFTapr} 
can be written as 
$\zb A \approx \zb B \zb F \zb D$ with a diagonal matrix $\zb
D=\diag(\hat \varphi(\zb r))$,
an oversampled Fourier matrix $\zb F$, and
a sparse matrix $\zb B$
with nonzero entries $b_{j,l}=\psi (x_j - l/ (\alpha N))$, where
$\psi$ approximates the window  function $\varphi$ and $\alpha$
denotes the oversampling factor. The generalisation to higher
dimensions is straightforward. A unified approach to the
efficient computations of \eqref{trigPoly} was suggested in \cite{st97,
  postta01}. 


Our algorithms are based on the approach in \cite{postta01} where one can change 
the window function $\varphi$ in a very simple way. Note, that
\begin{enumerate}
\item For a fixed oversampling factor $\alpha >1$ the approximation error introduced by the NFFT
decays exponentially with the window-width $m$.
\item The arithmetical complexity of the NFFT is ${\cal O} ((\alpha N)^d\log N+m^d M)$. 
\end{enumerate}
The manual \cite{kupo04b} and the appendix in \cite{post02} present details
for a suitable choice of the window function, the window-width, and the
oversampling factor with respect to accuracy, speed and memory usage.

In various papers, different window functions $\varphi$ for the NFFT
were considered. 
Furthermore, special approaches based on scaling vectors
\cite{NgLi99}, based on minimising the Frobenius norm of certain error
matrices \cite{st01}, or based on min-max interpolation \cite{fesu02} are
proposed. However, the numerical results in \cite{four, st01, fesu02} show that
these approaches are not superior to the approach based on the Kaiser-Bessel
window function.
We remark further that the NFFT  can realized
with all suggested window functions. However the accuracy depends on
the truncation parameter $m$. In our further numerical test we use the
Kaiser-Bessel window since this window provides the best results for a
particular chosen parameter $m$.

The adjoint algorithm, i.e. the evaluation of the sums
\begin{equation} \label{trigPolyT}
\zb {\hat f}_{\zbs r} := \sum_{j=0}^{M-1} f_j \eim{\zbs r \zbs x_j}\,,
\end{equation}
can be computed by the matrix vector multiplication with $\zb A^{\adj}$, where
we obtain a fast algorithm by using $\zb A^{\adj} \approx \zb D^{\top}
\zb F^{\adj} \zb B^{\top}$.
It was already pointed out in \cite{postta01,SaBeCo01}, that the gridding
method is simply a fast algorithm for the multiplication of the matrix
$\zb A^{\adj}$ with a vector.
Including a sampling density compensation yields the following gridding
algorithm, see also \cite{scsc, Ja, Pe}:
\begin{enumerate}
\item sampling density compensation, i.e., multiplication with a diagonal
  matrix $\zb W$,
\item approximation to an oversampled Cartesian grid, i.e., multiplication
  with the matrix $\zb B^{\top}$,
\item inverse fast Fourier transform, i.e., mult. with $\zb F^{\adj}$,
\item roll-off correction, i.e., multiplication with $\zb D^{\top}$.
\end{enumerate}
Note, that the role of the window function, appearing in the matrices $\zb D$
and $\zb B$, and the sampling density compensation, appearing in $\zb W$,
should not be mixed up.
It is widely accepted that a gridding method derived from
\eqref{approx} where the window function is the Kaiser-Bessel function
leads to the most efficient algorithms. 

Hence our aim is in the following to use this approximation and derive
methods for the fast matrix vector multiplication for the matrix $\zb H$ based
on \eqref{approx}. 

Since the support of $\tilde\psi$ is ${\rm supp} \tilde\psi\subset
[-m/(\alpha N), m/(\alpha N)]$ we can avoid the periodisation of $\psi$ 
if $x\in[-\frac{1}{2}+\frac{m}{\alpha N},\frac{1}{2}-\frac{m}{\alpha N}]$ 
and we obtain from \eqref{approx} and \eqref{FK=ck}
\begin{equation} \label{approx2}
\eip{kx}
\approx
\frac{1}{\alpha N \hat\varphi(k)}
\sum_{l=-\alpha N/2}^{\alpha N/2-1} \psi (x-\frac{l}{\alpha N})
\eim{\frac{kl}{\alpha N}},\quad
\end{equation}
where $k$ is now not restricted to integers but from the interval
$k\in [-N/2,N/2]$.
Note that \eqref{approx2} is a good approximation if 
$kx\in[-\frac{N}{4}+\frac{m}{2\alpha},-\frac{N}{4}+\frac{m}{2\alpha}]$.


\section{Fast matrix vector multiplication}\label{Sec:H}
 
There are different possibilities to realize a matrix vector
multiplication with $\zb H$.

\subsection{Method based on fast Fourier transforms for
   nonequispaced data in time {\bf and} frequency domain (3d NNFFT)}
  
A straightforward idea is to embed the data points in a higher
   dimensional space, i.e., set $\zb K_j= ((\zb k(t_j))^{\top},
   t_j)^{\top}\in \mathbb{R}^3$ $(j=0,\ldots, M-1)$ and $\zb R= ((\zb
   r)^{\top}, 
   \omega(\zb r))^{\top}\in \mathbb{R}^3$ $(\zb r\in I_{\zbs N}^2)$ (see
   \cite[Example 2]{LeGr05}). Fast algorithms for the multiplication
   with the matrix $ 
  \left(\eip{\zbs K_j\zbs R}\right)_{j=0,\hdots,M-1;\;\zbs r\in 
  I_{\zbss N}^2}$ are known as fast Fourier transforms for
   nonequispaced data in time and frequency domain. These algorithms were
   first suggested in \cite{duro93} and algorithms with error
   estimates and a precise choice of the incorporated constants were
   given in \cite{ElSt}. For the multivariate setting see
   \cite{postta01}. However we remark that this algorithms needs a
   further oversampling factor and is more expensive than the NFFT.

\subsection{Method based on fast Fourier transforms for
   nonequispaced data (3d NFFT)}

We suggest a much simpler and faster method based on the NFFT.
First of all we choose $N_3$ such that 
for all $\zb r\in I_{\zbs N}^2$, $j=0,\ldots,M-1$
\begin{equation}\label{wahlN3}
\omega(\zb r)t_j
\in[-\frac{N_3}{4\alpha}+\frac{m}{2\alpha},-\frac{N_3}{4\alpha}+\frac{m}{2\alpha}] 
\end{equation}
and further a scaling parameter $W$ such that
\[
\frac{\omega(\zb r)}{W}
\in[-\frac{1}{2}+\frac{m}{N_3},-\frac{1}{2}+\frac{m}{N_3}]\, . 
\]
Now we use the approximation \eqref{approx2} and obtain
\begin{eqnarray} \label{appr_inh}
&&{\rm e}^{\ti t_j \omega(\zbs r)}  =
\eip{\frac{W t_j}{2\pi} \frac{\omega(\zbss r)}{W}}\\ \nonumber
&\approx&
\frac{1}{N_3\hat\varphi\left(\frac{Wt_j}{2\pi}\right)}
\sum_{r_3=-N_3/2}^{N_3/2-1} \psi \left(\frac{\omega(\zb
  r)}{W}+\frac{r_3}{N_3}\right)  \eip{\frac{t_jW}{2\pi N_3}r_3}\, .
\end{eqnarray}
Using this approximation in \eqref{inverse2} we obtain
\begin{equation*}
\tilde{\zb s}(\zb k(t_j))\approx
\frac{1}{N_3 \hat\varphi\left(\frac{Wt_j}{2\pi}\right)}
\sum_{r_3=-N_3/2}^{N_3/2-1}
\sum_{\zbs r \in I^2_{\zbss N}} 
p(\zb r) \psi \left(\frac{\omega(\zb r)}{2\pi W}+\frac{r_3}{N_3}\right)
\end{equation*}
\begin{equation*}
\times \eip{\left(\begin{array}{c}
\zbs k(t_j)\\
\frac{t_jW}{2\pi N_3}
\end{array}\right)
\cdot
\left(\begin{array}{c}
\zb r\\
r_3
\end{array}\right)
}
\end{equation*}
and further by using $\zb k_j:= ((\zb k(t_j))^{\top},
  t_jW/(2\pi N_3))^{\top}\in \mathbb{R}^3$ $(j=0,\ldots, M-1)$;
$P(\zb r,r_3):= p(\zb r) \psi \left(\frac{\omega(\zbs
  r)}{W}+\frac{r_3}{N_3}\right)$ 
we obtain
\begin{equation*}
\tilde{\zb s}(\zb k(t_j))\approx
\frac{1}{N_3 \hat\varphi\left(\frac{Wt_j}{2\pi}\right)}
\sum_{\zbs r \in I^3_{\zbss N}} 
P(\zb r) \eip{\zbs k_j\zbs r}
\end{equation*}
where $I^3_{\zbs N} :=
\{-\frac{N_1}{2},\dots,\frac{N_1}{2}-1\}\times\{-\frac{N_2}{2},\dots,
\frac{N_2}{2}-1\}\times\{-\frac{N_3}{2},\dots,\frac{N_3}{2}-1\}$.
%Now let
%$\zb s:=\left(N_3 \hat\varphi\left(\frac{Wt_j}{2\pi}\right)\tilde{\zb s}(\zb
%  k(t_j))\right)_{j=0,\ldots, M-1}\in \mathbb{R}^M
%$
%and
%$
%\zb p :=\left(p(r_1,r_2) \psi
%\left(\frac{\omega(r_1,r_2)}{W}+\frac{r_3}{N_3}\right)\right)_{\zbs
%r\in I_N^3}\in 
%\mathbb{R}^{N_1N_2N_3}
%$
%and $\zb A$ the trivariate nonequispaced Fourier matrix.
Now the matrix vector product with $\zb H$ can be efficiently realised
by a trivariate NFFT.

We proceed and develop a fast matrix times vector
multiplication with $H^{\adj}$ more precisely we want to compute
\[
p_{\zbs r} =
\sum_{j=0}^{M-1} s_j 
{\rm e}^{-{\rm\scriptsize i}\omega(\zbs r)t}
{\rm e}^{-2\pi {\rm\scriptsize i}\zbs r \zbs k(t)}\, .
\]
Again we approximate ${\rm e}^{-{\rm\scriptsize i}\omega(\zbs r)t}$ by
\eqref{appr_inh} and get
\[
p_{\zbs r} \approx
\sum_{r_3=-N_3/2}^{N_3/2-1}\left(
\sum_{j=0}^{M-1} \frac{s_j}{N_3 \hat\varphi\left(\frac{Wt_j}{2\pi}\right)}
 \eim{\zbs k_j
\left(\begin{array}{c}
\zb r\\
r_3
\end{array}\right)}\right)
\]
\[
\times
\psi \left(\frac{\omega(\zb r)}{W}-\frac{r_3}{N_3}\right)
\]
The sum over $j$ can be computed in an efficient way by a
trivariate NFFT followed by a sparse sum over $r_3$.

\subsection{The 2d$\otimes$1d method}

After choosing $N_3$ as in \eqref{wahlN3} we choose a further scaling parameter $T$ such that
\[
\frac{t_j}{T}
\in[-\frac{1}{2}+\frac{m}{N_3},-\frac{1}{2}+\frac{m}{N_3}]\, . 
\]
Now we use the approximation \eqref{approx2} and obtain
\begin{eqnarray*}
&&{\rm e}^{\ti t_j \omega(\zbs r)} = 
\eip{\frac{T\omega(\zbss r)}{2\pi}\frac{t_j}{T}}\\ \nonumber
&\approx&
\frac{1}{N_3\hat\varphi\left(\frac{T\omega(\zbs r)}{2\pi}\right)}
\sum_{r_3=-N_3/2}^{N_3/2-1} 
\psi\left(\frac{t_j}{T}+\frac{r_3}{N_3}\right)  
\eip{\frac{T\omega(\zbss r)}{2\pi N_3}r_3}\, .
\end{eqnarray*}
Using this approximation in \eqref{inverse2} we obtain
\begin{equation*}
\tilde s(\zb k(t_j)) \approx
\sum_{r_3=-N_3/2}^{N_3/2-1}
\psi\left(\frac{t_j}{T}+\frac{r_3}{N_3}\right) \times
\end{equation*}
\begin{equation*}
\sum_{\zbs r \in I^2_{\zbss N}} 
\left(
\frac{p(\zb r)}{N_3\hat\varphi\left(\frac{T\omega(\zbs
      r)}{2\pi}\right)}
\eip{\frac{T\omega(\zbss r)}{2\pi}r_3}
\right)
{\rm e}^{2\pi {\rm\scriptsize i}\zbs r \zbs k(t_j)}\, .
\end{equation*}
We can perform  matrix-vector product with $\zb H$ by $N_3$ bivariate
NFFT followed by a sum over $r_3$.

Note that this method is known as ``time segmentation'' \cite{SuNoFe03}. The
idea is to use  small time segments over which $t$ is approximately
constant \cite{NoMe91}. With our unified approach there is no need to
compute coefficients in a precomputation step (see \cite{SuNoFe03}). 

Summary: We have suggested three different methods for matrix vector
multiplication with $\zb H$. Note the for the arithmetical complexity
for the 3d NNFFT and the 3d NFFT is ${\cal
  O}(N_1N_2N_3\log(N_1N_2N_3))$ but for the 2d$\otimes$1d method only 
 ${\cal  O}(N_1N_2N_3\log(N_1N_2))$. Furthermore the memory
 requirements for the algorithms are different. Hence we compare these
 algorithms in the following section.
 
%----------------------------------------------------------------------------
\section{Numerical results}\label{Sec:Num}
All algorithms were implemented in Matlab\&C and tested on a AMD Athlon(tm) XP
2700+, 2GB memory, SuSe-Linux, kernel 2.4.20-4GB-athlon, FFTW3.0.1, and NFFT2.


\begin{figure*}[ht] 
\centering
\begin{tabular}{ccc}
\includegraphics[height=4.7cm]{pics/spiral.jpg} &
\includegraphics[height=4.7cm]{pics/phantom_original.jpg} &
\includegraphics[height=4.7cm]{pics/fieldmap_simulated.jpg}
\end{tabular}
\caption{A subset of the k-Space trajectory, here with only 4
  interleaved spiral arms (left), the used phantom (middle) and the
  simulated field map (right)} 
\label{Fig:Phantom}
\end{figure*}

The reconstruction algorithms were tested with a $2d$-Shepp-Logan
phantom, cf. Figure \ref{Fig:Phantom}, of size $256\times256$ with a
simulated field map. Furthermore we use MR measurements of a physical
phantom by the Philips Achieva 1.5T device with a measured field
maps. In all test we used an interleaved spiral k-space trajectory,
given by 
\begin{equation*}
\zb {\tilde k}_j = A \sqrt{\frac{j}{M_1}} \left(\cos\frac{2\pi \omega
    j}{M_1}, \sin\frac{2\pi \omega j}{M_1}\right)^{\top}. 
\end{equation*}

{\bf spirale analytisch???}

with 16 interleaved spiral arms and 4720 knots each.
% with the parameters ???? $M_1=N^2$, $A=0.5$, $\omega=\frac{25}{32}N$ and $j=0 \dots M_1-1$.
The length of the readout intervall was 19ms for each spiral arm with an echo time $T_E=2$ms.

In all tests we used an oversampling factor $\alpha \approx 1.5$ and 
$m=6$, such that $\alpha N_1=\alpha N_2 = 384$. The factor $N_3$
depends on the field map. For the simulated field map and for the
measured field map we choose $N_3=28$ and $N_3=48$, respectively. 

In order to take into account the local sampling density in k-space we use
weights $\zb W$ obtained as the area of the Voronoi cell around each
sample knot, see also \cite{RaPrSiBoEg99,qhull}. In \cite{KnKuPo} it was
shown, that this method leads to very good results. 

The NFFT2 library allows different flags to reduce computation time at
expense of memory. 

\begin{NumExp} \label{Exp:1}
First of all, we compare the different reconstruction methods with a
$2d$-Shepp-Logan phantom. The simulated MR data are computed by the
$3d$-NFFT-method using the following field map 
\begin{equation*}
\omega(r_1,r_2) =
\left(\frac{r_1}{N}\right)^2+\left(\frac{r_2}{N}\right)^2
-\frac{1}{4}. 
\end{equation*}
Thus the field map is in the interval $[-\frac{1}{4},\frac{1}{4}]$. 
Table \ref{Tab:1} shows the signal-to-noise-ratio  
${\rm SNR}(\zb{\tilde p},\zb p):=20 \log_{10}({\|\zb p\|_2}\,/\,{\|\zb p-
\zb{\tilde p}\|_2})$ where $\zb {\tilde p}$ is our reconstruction and $\zb p$
denotes the original image. 
With gridding we denote an algorithm based on 2d NFFT where the field
inhomogeneities are ignored.  
The SNR for 3d NNFFT, 3d NFFT and
2d$\otimes$1d NFFT is nearly the same. 
Thus as expected we see that no method is rising above another in
image quality.
Figure \ref{Fig:1} and \ref{Fig:2} show the reconstructed Shepp-Logan
phantom with different methods after one iteration. Furthermore Figure
\ref{Fig:2} shows the  reconstructed phantom with the 2d$\otimes$1d
NFFT method after 5 and 10 iterations. 
The pictures show again that no reconstruction methods 3d NNFFT, 3d
NFFT and 2d$\otimes$1d NFFT  has advantages in image quality. Compared
to gridding, all methods yield 
to very good results. The image in \ref{Fig:1} shows the result if the
field inhomogeneities are ignored (gridding).  
\end{NumExp}

\begin{table}[ht]
\begin{center}
\begin{tabular}{|l||c|c|c|c|} \hline
 \rule{0mm}{4mm} method $\backslash$ iter. & $1$ & $2$  & $5$  & $10$   \\[1ex] \hline\hline
 \multicolumn{5}{|c|}{\rule{0mm}{4mm} SNR}\\[1ex] \hline\hline
 \rule{0mm}{4mm} gridding & 4.21 & 4.47 &  4.51 & 4.55  \\[1ex] \hline
 \rule{0mm}{4mm} 3d NNFFT & 10.15 & 13.60 & 14.13 & 14.56  \\[1ex] \hline
 \rule{0mm}{4mm} 3d NFFT & 10.15 & 13.60 & 14.13 & 14.56  \\[1ex] \hline
 \rule{0mm}{4mm} 2d$\otimes$1d NFFT & 10.15 & 13.60 & 14.13 & 14.56\\[1ex] \hline 
\end{tabular}
\end{center}
\caption{SNR for different reconstruction methods after $1$, $2$, $5$, and $10$ iterations.} \label{Tab:1}
\end{table}

\begin{figure*}[ht!]
\centering
\begin{tabular}{ccc}
\includegraphics[width=5.5cm]{pics/phantom_gridding_iter=1.jpg} &
\includegraphics[width=5.5cm]{pics/phantom_nnfft_iter=1.jpg}&
\includegraphics[width=5.5cm]{pics/phantom_3d_iter=1.jpg}\\
\includegraphics[width=5.5cm]{pics/phantom_gridding_iter=1row.jpg} &
\includegraphics[width=5.5cm]{pics/phantom_nnfft_iter=1row.jpg}&
\includegraphics[width=5.5cm]{pics/phantom_3d_iter=1row.jpg} 
\end{tabular}
\caption{The Shepp-Logan phantom (top) and the profile of the $128$th row of this (bottom), reconstructed
with {\bf gridding} (left), {\bf 3d NNFFT} (middle) and {\bf 3d NFFT} (right) after one iteration.} \label{Fig:1}
\end{figure*}


\begin{figure*}[ht!]
\centering
\begin{tabular}{ccc}
\includegraphics[width=5.5cm]{pics/phantom_2d1d_iter=1.jpg} &
\includegraphics[width=5.5cm]{pics/phantom_2d1d_iter=5.jpg}&
\includegraphics[width=5.5cm]{pics/phantom_2d1d_iter=10.jpg}\\
\includegraphics[width=5.5cm]{pics/phantom_2d1d_iter=1row.jpg} &
\includegraphics[width=5.5cm]{pics/phantom_2d1d_iter=5row.jpg} &
\includegraphics[width=5.5cm]{pics/phantom_2d1d_iter=10row.jpg}
\end{tabular}
\caption{The Shepp-Logan phantom (top) and the profile of the $128$th row of this (bottom), reconstructed
with {\bf 2d$\otimes$1d NFFT}: from left to right after 1,5 and 10 iterations.} \label{Fig:2}
\end{figure*}



\begin{NumExp} \label{Exp:2}
Now we compare the computation time and the memory usage of the
algorithms. Table \ref{Tab:2} shows the results for the same
reconstruction object as in Example \ref{Exp:1}. In both category's,  
time and space requirements, the 
3d NFFT beats the 3d NNFFT method. At first glance the 2d$\otimes$1d
NFFT method is the slowest but with the lowest memory usage. The
2d$\otimes$1d NFFT required almost the same memory usage as
gridding. In the following
Experiment \ref{Exp:3} we see the possibility to speed up the
2d$\otimes$1d NFFT method at
expense of memory. Thus to choose the fastest method, the available
memory must be considered. 
\end{NumExp} 

\begin{table}[ht]
\begin{center}
\begin{tabular}{|l||c|c|c|c|c|} \hline
 \rule{0mm}{4mm} method $\backslash$ iter. & $1$ & $2$  & $5$  & $10$ &  \\[1ex] \hline\hline
 \multicolumn{5}{|c|}{\rule{0mm}{4mm} CPU-Time} & MB\\[1ex] \hline\hline
 \rule{0mm}{4mm} gridding & 5.07 & 7.61 & 15.24 & 27.95 & 29 \\[1ex] \hline
 \rule{0mm}{4mm} 3d NNFFT & 144.75 & 217.16 & 434.26 & 796.18& 599 \\[1ex] \hline
 \rule{0mm}{4mm} 3d NFFT & 84.55 & 126.68 & 253.20 & 464.14 & 189\\[1ex] \hline
 \rule{0mm}{4mm} 2d$\otimes$1d NFFT & 159.00 & 237.93 &  475.50 & 871.65 & 32\\[1ex] \hline

\end{tabular}
\end{center}
\caption{CPU-time for different reconstruction methods after $1$, $2$, $5$, and $10$ iterations.} \label{Tab:2}
\end{table}

Because the reconstruction algorithms have
significantly different memory usage we tested different flags
in the following Example \ref{Exp:3}.  

\begin{NumExp} \label{Exp:3}
The NFFT2 library allows different flags to optimise the computing time which
affect the storage of the precomputed values for the third step within the
NFFT, i.e., the left factor $\zb B$, see Section \ref{Sec:NFFT}.
The flag PRE\_FULL\_PSI stores all nonzero entries of the matrix $\zb B$, 
PRE\_PSI in a lossless compressed form and PRE\_LIN\_PSI in a lossy compressed, trajectory
independent form. PRE\_PSI needs  ${\cal O} (d(2m+2))$, PRE\_FULL\_PSI ${\cal O} ((2m+2)^d)$ extra
memory for every knot $x_j$, $j=0,\hdots,M-1$.
For better comparison we have chosen the same
flag PRE\_PSI for all reconstruction methods in Example \ref{Exp:1} and \ref{Exp:2}.
Table \ref{Tab:3} shows CPU-Times and memory requirement of the
2d$\otimes$1d NFFT method with different flags. 
Again we use the Shepp-Logan phantom as in Example \ref{Exp:1}. 
If it is possible the flag PRE\_PSI is recommend. The ratio between Time speedup and extra used 
memory is good.
The flag PRE\_FULL\_PSI gives another great speedup but increases the extra used memory highly.
We remark that we were not able to use the flag PRE\_FULL\_PSI with the 3d NNFFT or 
3d NFFT method, because the memory requirements is greater than the 2GB of our machine.
\end{NumExp}

\begin{table}[ht]
\begin{center}
\begin{tabular}{|l||c|c|c|c|c|} \hline
 \rule{0mm}{4mm} method $\backslash$ iter. & $1$ & $2$  & $5$  & $10$ &  \\[1ex] \hline\hline
 \multicolumn{5}{|c|}{\rule{0mm}{4mm} CPU-Time} & Mem in MB\\[1ex] \hline\hline
 \rule{0mm}{4mm} PRE\_LIN\_PSI & 288.47 & 432.04 & 862.95 & 1580.92& 17 \\[1ex] \hline
 \rule{0mm}{4mm} PRE\_PSI &144.75 & 217.16 & 434.26 & 796.18& 32\\[1ex] \hline
 \rule{0mm}{4mm} PRE\_FULL\_PSI & 51.91 & 77.90 & 155.84 & 285.91 & 185\\[1ex] \hline
\end{tabular}
\end{center}

\caption{CPU-time for different flags with the 2d$\otimes$1d NFFT
 reconstruction methods after $1$, $2$, $5$, and $10$ iterations.}
 \label{Tab:3} 
\end{table}

\begin{NumExp} \label{Exp:4}
In the last Example we test our algorithms with MR measurements of a
physical phantom by the Philips Achieva 1.5T device.


{\bf Wie wurde die fieldmap berechnet? siehe Fessler?}

The field map is shown in Figure \ref{Fig:fieldmap}. The off-resonance
is in the interval $[-0.475, 0.591]$ kHz, which  
causes ??? $\pi$ extra spin phase accural during the readout.
Figure \ref{Fig:Philips} shows images reconstructed with gridding
(left) and with 2d$\otimes$1d NFFT (right). 
\end{NumExp}

\begin{figure}[ht!] 
\begin{center}
\includegraphics[width=8cm]{pics/fieldmap.jpg} 
\end{center}
\caption{The measured field map in kHz.}
\label{Fig:fieldmap}
\end{figure}



\begin{figure*}[ht] 
\centering
\begin{tabular}{cc}
\includegraphics[width=8cm]{pics/reallife_600_gridding_iter=1.jpg} &
\includegraphics[width=8cm]{pics/reallife_600_iter=1.jpg}\\
\includegraphics[width=8cm]{pics/reallife_600_gridding_iter=1row.jpg} &
\includegraphics[width=8cm]{pics/reallife_600_iter=1row.jpg}
\end{tabular}
\caption{Physical phantom from a MR scan by the Philips Achieva 1.5T device (top) and the profile of the $128$th row of this (bottom), reconstructed
with gridding (left) and 2d$\otimes$1d NFFT (right) after one iteration.}
\label{Fig:Philips}
\end{figure*}



\section{Discussion}\label{Sec:Dis}
la, la, ...

We are concerned with the application to simulated as well as to real life data, the reconstruction
quality, and the usage of time and memory resources.

{\bf Acknowledgement.}

%\bibliographystyle{IEEEabbrv}
\bibliographystyle{IEEEtran}
\bibliography{ref}
\end{document}
