%=============================================================================
%\documentclass[11pt,twoside]{article}
\documentclass[11pt,a4paper,twoside,bibtotoc]{scrartcl}
%\documentclass[twoside,11pt]{article}
%=============================================================================
\usepackage{a4wide}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{subfigure}
\usepackage{natbib}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{color}
%\usepackage{showkeys}
%============================================================================
\renewcommand{\topfraction}{1}
\renewcommand{\textfraction}{0}
\setcounter{totalnumber}{4}
%============================================================================
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


\newcommand{\N}{\ensuremath{\mathbb{N}}}
\newcommand{\NZ}{\ensuremath{\mathbb{N}_{0}}}
\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\C}{\ensuremath{\mathbb{C}}}

\newcommand{\adj}{{\vdash \hspace*{-1.72mm} \dashv}}
\newcommand{\supp}{\:{\rm supp}}
\newcommand{\Ln}[2]{\ensuremath{\text{\rm{L}}^{#1}\left(#2\right)}}
\newcommand{\fun}[2]{\ensuremath{#1{\hspace{-0.4ex}}\left(#2\right)}}
\newcommand{\dx}{\text{\rm{d}}}
\newcommand{\e}{\mbox{\rm{e}}}
\newcommand{\im}{\mbox{\scriptsize\rm{i}}}
\newcommand{\diag}{\text{\rm{diag}}}
\def\invisible#1{\textcolor{white}{#1}}

\numberwithin{equation}{section}
\numberwithin{table}{section}
\numberwithin{figure}{section}
%============================================================================
\title{Fast summation of radial functions on the sphere}

\date{\today}

\author{
Jens Keiner\thanks{keiner@math.uni-luebeck.de, University of
  L\"ubeck, Institute of Mathematics, 23560 L\"ubeck, Germany} \and
Stefan Kunis\thanks{kunis@math.uni-luebeck.de, University of
  L\"ubeck, Institute of Mathematics, 23560 L\"ubeck, Germany} \and
Daniel Potts\thanks{potts@math.uni-luebeck.de, University of
  L\"ubeck, Institute of Mathematics, 23560 L\"ubeck, Germany} 
}
%=============================================================================
\begin{document}

\maketitle

\begin{abstract}
\medskip
%\noindent
%2000 {\it Mathematics Subject Classification}. 65F10, 65F15, 65T40.
\noindent
{\it Radial functions are a powerful tool in many areas of multi-dimensional 
approximation. We present a fast approximative algorithm for the evaluation of
linear combinations of radial functions on the sphere $\mathbb{S}^2$. The 
approach is based on a rank approximation to the corresponding Gramian matrix
and fast algorithms for spherical Fourier transforms. We establish explicit error 
bounds and provide numerical examples.}
\end{abstract}

%=============================================================================
\section{Introduction}\label{sect:1}
%=============================================================================
Radial basis functions are a powerful tool in many areas of multi-dimensional 
approximation and interpolation.
In radial basis function methods $\in \R^3$ one approximates functions
from $\R^3 
\rightarrow \R$ by linear combinations of radial symmetric translates 
$\fun{\Phi}{\mathbf{y} - \cdot \,}$, $\mathbf{y} \in \R^3$, of a single function $\phi : \R^{+} \rightarrow \R$, i.e. 
\[
  \fun{\Phi}{\mathbf{y} - \cdot \,}:\R^3 \rightarrow \R, \, \mathbf{x} \mapsto \phi(\|\mathbf{y} - \mathbf{x}\|_2).
\]
The spherical counterpart are the \emph{zonal functions} which depend solely
on the geodesic distance of two points on the sphere $\mathbb{S}^2:=\{
\mathbf{x} \in \mathbb{R}^3: \|\mathbf{x}\|_2=1\} \subset \R^3$. The norm $\|\mathbf{y} - \mathbf{x}\|_2$ 
is replaced by the usual inner product.

More formally, let $K \in \Ln{2}{[-1,1]}$ and define for fixed
$\mathbf{\eta} \in \mathbb{S}^2$ the $\mathbf{\eta}$-zonal function 
\[
  \fun{K}{\mathbf{\eta} \: \cdot \,}: \mathbb{S}^2 \rightarrow \R,\ \mathbf{\xi} \mapsto
  \fun{K}{\mathbf{\eta} \cdot \mathbf{\xi}}\,.% \qquad \mathbf{\xi} \in \mathbb{S}^2.
\]
Note that in spherical coordinates, a point $\mathbf{\xi} \in \mathbb{S}^2$ is identified with 
a vector $(\vartheta,\varphi) \in [0,\pi] \times [-\pi,\pi)$. 
Then the {\it spherical distance} between the two points 
$\eta=(\theta,\varphi)$ and $\xi=(\theta',\varphi')$ of $\mathbb{S}^2$ is
determined by
$
%\eta \xi :=
{\rm arccos}\;  (\eta\cdot \xi)                     
$
with the (cartesian) inner product 
$$
\eta\cdot \xi = \cos\theta\; \cos \theta^\prime \; +\; \sin\theta\; \sin
\theta^\prime\;\cos(\varphi-\varphi^\prime)\, .
$$

The formal description of the problem we will address is as follows:
Given $D,L \in \N$, a set of arbitrary \emph{source nodes} $\mathcal{Y} :=
\{\mathbf{\eta}_{l} \in \mathbb{S}^2: l = 0,\ldots,L-1\}$, and a vector of
real coefficients $\mathbf{b}:=(b_{l})_{l=0}^{L-1}$, evaluate the sum
\begin{equation}
  \label{Applications:KernelSum}
  \fun{f}{\mathbf{\xi}} := \sum_{l = 0}^{L-1} b_{l} \fun{K}{\mathbf{\eta}_{l} \cdot \mathbf{\xi}}
\end{equation}
on a set of arbitrary \emph{target nodes} $\mathcal{X} := \{\mathbf{\xi}_{d}
  \in \mathbb{S}^2: d=0,\ldots,D-1\}$.

The naive approach for evaluating \eqref{Applications:KernelSum} leads to
an $\mathcal{O}\left(LD\right)$ algorithm if we assume that the zonal function
$\fun{K}{\mathbf{\eta} \: \cdot}$ can be evaluated in constant time or that all values
$\fun{K}{\mathbf{\eta}_{l} \cdot \mathbf{\xi}_{d}}$ can be stored in advance. 
For large $L$ and $D$ the computational effort becomes quickly unaffordable.
We present an approximative algorithm with arithmetic complexity 
${\cal O} (D+L)$ based on the \emph{nonuniform fast spherical Fourier 
transform $($NFSFT$)$}, cf. \cite{kupo02,keiner05}. 

The \emph{panel clustering method} on the sphere in \cite{FrGlSch98}
reduces the 
computational effort for evaluating \eqref{Applications:KernelSum} based on the
traditional method of dividing the evaluation into a near- and a far-field:
For every zonal function $\fun{K}{\mathbf{\eta}_l \: \cdot}$, the near-field
contribution is calculated exactly whereas the contribution of the far-field
is approximated coarsely. However our  algorithm, is simpler structured 
and can be easily adapted to different kernels $K$.
Our method utilises the widely known FFT and can consequently incorporate
advanced FFT implementations.
We prove error estimates to obtain clues about the choice of the
involved parameters and present numerical examples.

The remainder of this paper is organised as follows:
To keep the text self-contained, we summarise definitions and properties
of Fourier expansions of zonal functions in Section 2.
In Section 3, we propose our fast scheme for the evaluation of
\eqref{Applications:KernelSum}, whereas Section 4 establishes
error estimates for a variety of functions $K$.
Finally, Section 5 presents numerical results.

%=============================================================================
\section{Prerequisites} \label{sect:2}
%=============================================================================
The \emph{Legendre polynomials} $P_k : [-1,1] \rightarrow \R$, $k \in
\N_{0}$, as classical orthogonal polynomials are given by their
\emph{Rodrigues formula} 
\[
\fun{P_k}{x} := \frac{1}{2^k k!} \frac{\dx^k}{\dx x^k} \left(x^2-1\right)^k.
\]
One verifies
\begin{equation}\label{leg_prop}
\fun{P_{k}}{\pm1} = \left(\pm1\right)^{k}, \quad
\max_{x \in [-1,1]} \left|\fun{P_{k}}{x}\right| = 1,\text{ and} \quad
\left|\fun{P_{k}}{\cos\vartheta}\right| \le \sqrt{\frac{2}{\pi k
    \sin\vartheta}},
\end{equation}
where the last inequality holds for $\vartheta \in (0,\pi)$ and $k \ge 1$, see
e.g. \cite[pp. 47]{niuv}.
Furthermore, two recurrence relations are given by
\begin{equation}\label{three1}
\left(k+1\right)\fun{P_{k+1}}{x} = \left(2k+1\right)x\fun{P_{k}}{x} - k\fun{P_{k-1}}{x}
\end{equation}
and
\begin{equation}\label{three2}
\left(2k+1\right) \fun{P_{k}}{x} = \fun{P_{k+1}'}{x} - \fun{P_{k-1}'}{x}.
\end{equation}
The more general \emph{associated Legendre functions} $P_{k}^n$, $k \in \NZ$, $n \le k$, are defined by
\[
  \fun{P_{k}^n}{x} := \left(\frac{(k-n)!}{(k+n)!}\right)^{1/2} 
  \left(1-x^2\right)^{n/2} \frac{\dx^n}{\dx x^n} \fun{P_{k}}{x}.
\]

Let the space of real-valued continuous functions on the sphere be decomposed
into the direct sum of spaces of \emph{spherical harmonics}, i.e.
$C(\mathbb{S}^2)=\bigoplus_{k=0}^{\infty} \mathcal{H}_k$. We denote by 
$\left\{Y_{k}^n\right\}_{k \in \NZ; n=-k,\ldots,k}$ the 
standard $\text{L}^2$-orthonormal basis of spherical harmonics given by
\[
  \fun{Y_{k}^n}{\mathbf{\xi}} = \fun{Y_{k}^n}{\vartheta,\varphi} := 
  \sqrt{\frac{2k+1}{4\pi}} 
  \fun{P_{k}^{|n|}}{\cos\vartheta} \e^{\im n \varphi}.
\]
The orthogonal expansion in terms of the basis functions $Y_k^n$ 
of the zonal function $\fun{K}{\mathbf{\eta} \: \cdot \,}$ is given by
\begin{equation}
  \label{equation:kernelExpansion}
  \fun{K}{\mathbf{\eta} \cdot \mathbf{\xi}} = \sum_{k = 0}^{\infty} \sum_{n=-k}^k
  \fun{K^{\wedge}}{k}\overline{\fun{Y_{k}^n}{\mathbf{\eta}}} \fun{Y_{k}^n}{\mathbf{\xi}}
  = \sum_{k = 0}^{\infty} \frac{2k+1}{4\pi} \fun{K^{\wedge}}{k}
  \fun{P_{k}}{\mathbf{\eta} \cdot \mathbf{\xi}}.
\end{equation}
Here, we use the addition theorem
\[
\sum_{n=-k}^{k} \overline{\fun{Y_{k}^n}{\mathbf{\eta}}} \fun{Y_{k}^n}{\mathbf{\xi}} =
    \frac{2k+1}{4\pi}\fun{P_k}{\mathbf{\eta} \cdot \mathbf{\xi}}
\]
and the \emph{Fourier-Legendre coefficients} $\fun{K^{\wedge}}{k}$ of $K$ are given for $k \in \NZ$
by
\begin{equation}
  \label{equation:legtrafo}
  \fun{K^{\wedge}}{k} := 2 \pi \int_{-1}^{1} \fun{K}{x} \fun{P_{k}}{x} \dx 
  x\,.
\end{equation}

\begin{remark}
Of course, to every radial function $\phi(\|\mathbf{\eta} - \cdot \,\|_2)$ corresponds a zonal function 
$\fun{K}{\mathbf{\eta} \: \cdot \,}$ by means of 
\[
  \fun{K}{\mathbf{\eta} \cdot \mathbf{\xi}} := \phi\left(\sqrt{2-2\mathbf{\eta} \cdot
  \mathbf{\xi}}\right)\,.
\]
The relation between radial basis functions and their corresponding zonal
functions is considered in detail in \cite{CaFi}.
\end{remark}

%=============================================================================
\section{Fast Summation}
%=============================================================================
Basically, the first representation in \eqref{equation:kernelExpansion} allows
the construction of fast algorithms, owing to the separation of the
nodes $\mathbf{\eta}$ and $\mathbf{\xi}$.
We simply propose to truncate the series \eqref{equation:kernelExpansion} at a 
fixed \emph{cut-off degree} $M \in \NZ$, i.e.
\begin{equation}
  \label{Applications:TruncatedSeries}
  \fun{K}{\mathbf{\eta} \cdot \mathbf{\xi}} \approx \fun{K_{M}}{\mathbf{\eta} \cdot
  \mathbf{\xi}} := \sum_{k=0}^{M} \sum_{n=-k}^k \fun{K^{\wedge}}{k}
  \overline{\fun{Y_{k}^n}{\mathbf{\eta}}} \fun{Y_{k}^n}{\mathbf{\xi}}.
\end{equation}

Substituting \eqref{Applications:TruncatedSeries} into
\eqref{Applications:KernelSum} and interchanging the order of summation we
finally obtain the approximation
\begin{equation}
 \label{Eq:truncf_M}
  \fun{f}{\mathbf{\xi}} \approx \fun{f_{M}}{\mathbf{\xi}} := \sum_{k=0}^{M} \sum_{n=-k}^k \fun{K^{\wedge}}{k}
  \left(\sum_{l = 0}^{L-1} b_{l} \overline{\fun{Y_{k}^n}{\mathbf{\eta}_{l}}}\right)
  \fun{Y_{k}^n}{\mathbf{\xi}}
\end{equation}
to be evaluated at the $D$ target nodes $\mathbf{\xi}_{d}$.

Our algorithm now works as follows: The expression in the inner brackets 
can be evaluated by an \emph{adjoint nonuniform fast spherical Fourier 
transform (adjoint NFSFT)} with $\mathcal{O}(L + M^2 \log^2 M)$ arithmetic 
operations involving the $L$ source nodes $\mathbf{\eta}_{l}$. This is followed 
by $(M+1)^2$ multiplications with the Fourier-Legendre coefficients 
$\fun{K^{\wedge}}{k}$, and completed by a NFSFT to evaluate the
outer sum at the $D$ target nodes $\mathbf{\xi}_{d}$ with $\mathcal{O}\left(D + M^2 \log^2
M\right)$ arithmetic operations.

In Section \ref{Basics:SphericalKernels}, we will show that the choice 
of the cut-off degree $M$ only depends on the desired accuracy
and the particular zonal function $\fun{K}{\mathbf{\eta} \: \cdot \,}$, but neither
on the numbers $L$ and $D$ of source nodes $\mathbf{\eta}_{l}$ and target nodes 
$\mathbf{\xi}_{d}$, nor on their distribution.
In total, this yields an $\mathcal{O}\left(L + D\right)$ algorithm for evaluating the 
approximation $f_{M}$.

The nonuniform fast spherical Fourier transform algorithm (\cite{kupo02}) is 
a derivative of the stabilised Driscoll-Healy algorithm \cite{drhe,postta97}
combined with the fast Fourier transform for nonequispaced nodes (NFFT)
described for example in \cite{postta01}. Recently, the corresponding adjoint 
algorithm was derived and implemented (see \cite{keiner05}). The algorithms 
themselves are approximative, on the one hand owing to a thresholded 
stabilisation scheme, and on the other by using the approximative
NFFT algorithm.

For practical calculations, the Fourier-Legendre coefficients 
$\fun{K^{\wedge}}{k}$ have to be stored in advance in a pre-computation step. 
For some examples these coefficients are known explicitly whereas for others, one might use \eqref{equation:legtrafo}
and the recurrence relations \eqref{three1} and \eqref{three2} to derive
recursion formulae for the needed coefficients $\fun{K^{\wedge}}{k}$.
Discretising \eqref{equation:legtrafo} is a second possibility for obtaining them.
But here, we do not focus on the latter method and the error it would introduce, since for all Fourier-Legendre coefficients considered later on, an exact computation scheme is available.

\begin{remark}\label{rem:rankM}
In matrix-vector notation the original problem \eqref{Applications:KernelSum} reads $\mathbf{f}=\mathbf{K} \: \mathbf{b}$,
%\begin{equation}\label{eq:MV}
%\mathbf{f}=\mathbf{K} \mathbf{b}
%\end{equation}
where
\begin{align*}
  \mathbf{f} & := \left(\fun{f}{\mathbf{\xi}_{d}}\right)_{d=0}^{D-1} \in \R^D,\\
  \mathbf{K} & :=(\fun{K}{\mathbf{\eta}_{l} \cdot
  \mathbf{\xi}_d})_{d=0,\hdots,D-1;l=0,\hdots,L-1}\in \R^{D\times L}\,.
\end{align*}

Our approach is a particular rank $(M+1)^2$ approximation to the matrix $\mathbf{K}$ and takes the form
$
  \mathbf{f}_M = \mathbf{Y_{\mathcal{X}}} \: \mathbf{\hat W} \:
  \mathbf{Y_{\mathcal{Y}}}^{\adj} \: \mathbf{b}
$
with
\begin{align}
  \nonumber
  \mathbf{f}_M & := \left(\fun{f_M}{\mathbf{\xi}_{d}}\right)_{d=0}^{D-1} \in \R^D,
  \\ \nonumber
  \mathbf{Y_{\mathcal{X}}} & := \left(\fun{Y_k^n}{\mathbf{\xi}_{d}}\right)_{d=0,\ldots,D-1;
  k=0,\ldots,M,\:n=-k,\ldots,k} \in \C^{D \times
  \left(M+1\right)^2}, \\ \nonumber
  \mathbf{\hat W} & := \fun{\diag}{\mathbf{\hat w}},\ \mathbf{\hat w} := \left(\hat
  w_{k}^{n}\right)_{k=0,\ldots,M,\:n=-k,\ldots,k} \in \R^{(M+1)^2},\ \hat w_{k}^n :=
  \fun{K^{\wedge}}{k}, \\ \nonumber
  \mathbf{Y_{\mathcal{Y}}} & := \left(\fun{Y_k^n}{\mathbf{\eta}_{l}}\right)_{l=0,\ldots,L-1;
  k=0,\ldots,M,\:n=-k,\ldots,k} \in \C^{L \times \left(M+1\right)^2}.
\end{align}
\end{remark}

The proposed method is summarised in Algorithm \ref{Applications:Algorithm:FastSummation}.
% In summary, we propose the following algorithm:
\begin{algorithm}[ht]
  \caption{Fast summation of zonal functions}
  \label{Applications:Algorithm:FastSummation}    
  \begin{algorithmic}
    \STATE Input:             $L \in \N$, coefficients $\mathbf{b}:=\left(b_{l}\right)_{l=0}^{L-1}$, 
                              $b_{l} \in \R$, source nodes $\mathbf{\eta}_{l} \in \mathbb{S}^2$ for $l=0,
                              \ldots,L-1$, \\ 
    \STATE \invisible{Input:} $D \in \N$, target nodes $\mathbf{\xi}_{d} \in \mathbb{S}^2$ for $d=0,\ldots,D-1$,\\
    \STATE \invisible{Input:} $M \in \NZ$, Fourier-Legendre coefficients $\fun{K^{\wedge}}{k}$ for $k=0,\ldots,M$.  
    \STATE
    \STATE
    \STATE Compute $\mathbf{\tilde{a}} := \mathbf{Y_{\mathcal{Y}}}^{\adj} \: \mathbf{b}$ 
           with $\mathbf{\tilde{a}} := (\tilde{a}_k^n)_{k=0,\ldots,M, n=-k,\ldots,k}$ 
           by an adjoint NFSFT.
    \FOR {$k=0,\ldots,M$} 
      \FOR {$n=-k,\ldots,k$} 
        \STATE Set $a_{k}^n := \tilde{a}_{k}^n \: \fun{K^{\wedge}}{k}$.
      \ENDFOR
    \ENDFOR
    \STATE Compute $\mathbf{f}_M := \mathbf{Y_{\mathcal{X}}} \: \mathbf{a}$ with 
           $\mathbf{a} := \left(a_k^n\right)_{k=0,\ldots,M, n=-k,\ldots,k}$ 
           by a NFSFT.
    \STATE
    \STATE Output: $\left(\fun{f_M}{\mathbf{\xi}_{d}}\right)_{d=0}^{D-1}$ approximating
                    $\left(\fun{f}{\mathbf{\xi}_{d}}\right)_{d=0}^{D-1}$.
    \STATE
    \STATE Complexity: $\mathcal{O}\left(M^2 \log^2M + L + D\right)$.  
\end{algorithmic}
\end{algorithm}

\begin{remark}
Replacing the NFSFT algorithms by their slow versions \emph{nonuniform 
discrete spherical Fourier transform (NDSFT)} and \emph{adjoint NDSFT}, i.e. 
direct algorithms which need 
$\mathcal{O}\left(L M^2\right)$ and
$\mathcal{O}\left(D M^2\right)$ arithmetic operations for the multiplications with the 
matrices $\mathbf{Y}_{\mathcal{Y}}^{\adj}$ and $\mathbf{Y}_{\mathcal{X}}$, respectively, 
yields an $\mathcal{O}\left(L+D\right)$ algorithm, too. Nevertheless, the fast 
algorithms are the key for applications to large data sets since they 
decouple the cut-off degree $M$ from the numbers of nodes $L$ and $D$.
\end{remark}

%=============================================================================
\section{Error estimates and examples}\label{Basics:SphericalKernels}
%=============================================================================
Besides the well-known errors appearing in the NFFT computations, see
\cite{postta01}, our algorithm causes the following systematic error:
\begin{lemma}\label{lemma:error}
  The proposed approximation $f_{M}$ obeys the uniform error estimate
  \begin{equation*}
    \left\|f - f_{M}\right\|_{\infty} \le \left\|\mathbf{b}\right\|_1 \sum_{k>M}
    \frac{2k+1}{4\pi} \left|\fun{K^{\wedge}}{k}\right|.
  \end{equation*}
%  Moreover, if we assume random uniformly distributed source and target nodes, 
%  then we obtain the asymptotic bound
%  \begin{equation*}
%    \left\|\mathbf{f} - \mathbf{f}_{M}\right\|_{\infty} \le \left\|\mathbf{b}\right\|_1 
%    \frac{2 \fun{\Gamma^2}{\frac{3}{4}}}{4\pi^2} \left( 2 \sum_{k>M} 
%    \sqrt{k} \abs{\fun{K^{\wedge}}{k}} + \sum_{k>M} \frac{1}{\sqrt{k}} 
%    \abs{\fun{K^{\wedge}}{k}}\right).
%  \end{equation*}
\end{lemma}
\begin{proof}
  The assertion is obtained by
  \begin{equation*}
    \left\|f - f_{M}\right\|_{\infty} 
%     = \max_{\mathbf{\xi} \in \mathbb{S}^2} 
%        \left|\sum_{l=0}^{L-1} b_{l} \sum_{k>M} \frac{2k+1}{4\pi} \fun{K^{\wedge}}{k} 
%        \fun{P_{k}}{\mathbf{\eta}_{l} \cdot \mathbf{\xi}}\right|
     \le \sum_{l=0}^{L-1} \left| b_{l} \right| \max_{\mathbf{\xi} \in \mathbb{S}^2} \left| \sum_{k>M} 
          \frac{2k+1}{4\pi} \fun{K^{\wedge}}{k} \fun{P_{k}}{\mathbf{\eta}_{l} \cdot
    \mathbf{\xi}} \right|
%\\& \le \left\|\mathbf{b}\right\|_1 \sum_{k>M} \frac{2k+1}{4\pi} \abs{\fun{K^{\wedge}}{k}}
  \end{equation*}
  and the fact $\left|\fun{P_{k}}{x}\right| \le 1$. 
%%   If we now assume random uniformly 
%%   distributed source and target nodes, we might replace this estimate in the asymptotic sense, i.e.
%%   for $L,D \towards \infty$ by the expected value
%%   \begin{equation*}
%%     E_{k} := \int_{0}^{\pi} \fun{P_{k}}{\cos\vartheta} \fun{p}{\vartheta} \; \dx \vartheta,
%%   \end{equation*}
%%   where $\fun{p}{\vartheta} := \frac{\sin\vartheta}{2}$ is the probability for a source node 
%%   $\mathbf{\eta}$ and a target node $\mathbf{\xi}$ to span the angle $\vartheta$. 
%%   Now using the estimate
%%   $\left|\fun{P_{k}}{\cos\vartheta}\right| \le \sqrt{\frac{2}{\pi k \sin\vartheta}}$ for 
%%   $\vartheta \in \left(0, \pi\right)$ yields
%%   \begin{equation*}
%%     E_{k} 
%%     \le \int_{0}^{\pi} 
%%           \sqrt{\frac{2}{\pi k \sin\vartheta}} \frac{\sin\vartheta}{2} \; \dx \vartheta \\
%%       = \int_{0}^{\pi} \sqrt{\frac{\sin\vartheta}{2 \pi k }} \; \dx \vartheta \\
%%       = \frac{2 \fun{\Gamma^2}{\frac{3}{4}}}{\pi\sqrt{k}}.
%%   \end{equation*}
%%   We obtain finally
%%   \begin{align*}
%%     \left\|\mathbf{f} - \mathbf{f}_{M}\right\|_{\infty} 
%%     & \le \left\|\mathbf{b}\right\|_1 \frac{2 \fun{\Gamma^2}{\frac{3}{4}}}{\pi} \sum_{k>M} 
%%     \frac{2k+1}{4\pi\sqrt{k}} \abs{\fun{K^{\wedge}}{k}}\\
%%     & \le \left\|\mathbf{b}\right\|_1 \frac{2 \fun{\Gamma^2}{\frac{3}{4}}}{4\pi^2} \left( 2 \sum_{k>M} 
%%     \sqrt{k} \abs{\fun{K^{\wedge}}{k}} + \sum_{k>M} \frac{1}{\sqrt{k}} \abs{\fun{K^{\wedge}}{k}}\right)\\
%%   \end{align*}
%%   Now using the estimates
%%   $\left|\fun{P_{k}}{\cos\vartheta}\right| \le \sqrt{\frac{2}{\pi k \sin\vartheta}}$ for 
%%   $\vartheta \in \left[\arcsin \frac{2}{\pi k}, \pi - \arcsin \frac{2}{\pi k}\right]$ and 
%%   $\left|\fun{P_{k}}{\cos\vartheta}\right| \le 1$ for $\vartheta \in \left[0, \arcsin 
%%   \frac{2}{\pi k}\right)\cup \left( \pi-\arcsin\frac{2}{\pi k}, \pi\right]$ yields
%%   \begin{align*}
%%     E_{k} 
%%     & \le 2\arcsin\frac{2}{\pi k} + 
%%           \int_{\arcsin\frac{2}{\pi k}}^{\pi - \arcsin\frac{2}{\pi k}} 
%%           \sqrt{\frac{2}{\pi k \sin\vartheta}} \frac{\sin\vartheta}{2} \; \dx \vartheta \\
%%     &   = 2\arcsin\frac{2}{\pi k} +  \int_{\arcsin\frac{2}{\pi k}}^{\pi - \arcsin\frac{2}{\pi k}} 
%%           \sqrt{\frac{\sin\vartheta}{2 \pi k }} \; \dx \vartheta \\
%%     &   = 2\arcsin\frac{2}{\pi k} - \frac{4}{\sqrt{2\pi k}} \fun{E}{\frac{\arcsin\frac{2}{\pi k}}{2} - \frac{\pi}{4},2},
%%   \end{align*}
%%   and \fun{E}{\phi,j} is the elliptic integral of the second kind
%%   \begin{equation*}
%%     \fun{E}{\phi,j} := \int_{0}^{\phi} \sqrt{1-j^2\sin^2 \theta} \: \dx \theta.
%%   \end{equation*}
%%   We obtain finally
%%   \begin{equation*}
%%     \left\|\mathbf{f} - \mathbf{f}_{M}\right\|_{\infty} 
%%     \le \left\|\mathbf{b}\right\|_1 \left(2\sum_{k>M} \fun{\arcsin}{\frac{2}{\pi k}} \frac{2k+1}{4\pi} \abs{\fun{K^{\wedge}}{k}} - 
%%     \frac{4}{\sqrt{2\pi}} \sum_{k>M} \fun{E}{\frac{\arcsin\frac{2}{\pi k}}{2} - \frac{\pi}{4},2} \frac{2k+1}{4\pi\sqrt{k}} 
%%     \abs{\fun{K^{\wedge}}{k}}\right),
%%   \end{equation*}
\end{proof}

Thus, the error decouples into a part solely dependent on the coefficients
$b_l$ and a decay condition for the Fourier-Legendre coefficients 
$\fun{K^{\wedge}}{k}$ of the
particular zonal function $\fun{K}{\mathbf{\eta} \: \cdot \,}$.
General results for the decay of Fourier-Legendre coefficients are given in
\cite{Sc97,bahu01,CaFi}: For a fixed set of parameters, the coefficients
$\fun{K^{\wedge}}{k}$ obey a certain decay rate with respect to $k$.
Note furthermore that the spherical convolution lemma \cite[pp. 63]{frgesc} yields a simple
possibility to trade localisation of a zonal function in spatial domain
against the decay of its Fourier-Legendre coefficients, see \cite{Sc97} for
details.

In contrast to \cite{Sc97,bahu01,CaFi}, we use the decay of the
Fourier-Legendre coefficients with respect to $k$ and the parameters of the
zonal function $\fun{K}{\mathbf{\eta} \: \cdot \,}$ to determine the cut-off degree
$M$ in \eqref{Applications:TruncatedSeries}.
We consider a 'representative' range of zonal functions. A closed form expression
allows for direct evaluation and numerical verification. Depending on the 
particular zonal function $\fun{K}{\mathbf{\eta} \: \cdot \,}$, we use different 
methods for pre-computing the Fourier-Legendre coefficients $\fun{K^{\wedge}}{k}$.

\subsection{Poisson and Singularity kernel}
\begin{definition}\label{def:poisson_sing}
  Let $h \in (0,1)$, then the
  \begin{enumerate}
  \item \emph{Poisson kernel}
    $Q_{h}:[-1,1] \rightarrow \R$ is given by
    \[
    \fun{Q_{h}}{x} := \frac{1}{4\pi} \frac{1-h^2}{\left(1-2hx+h^2\right)^{3/2}}\,
    \]
  \item and the \emph{singularity kernel}
    $S_{h}:[-1,1] \rightarrow \R$ is given by
    \[
    \fun{S_{h}}{x} := \frac{1}{2\pi} \frac{1}{\left(1-2hx+h^2\right)^{1/2}}.
    \]
  \end{enumerate}
\end{definition}

We refer to Figure \ref{Basics:Figure:PoissonSingularityKernel}. The parameter 
$h$ allows for controlling the localisation of 
$\fun{Q_{h}}{\mathbf{\eta} \: \cdot \,}$ and $\fun{S_{h}}{\mathbf{\eta} \: \cdot \,}$ 
around $\eta \in \mathbb{S}^2$, respectively.
The Poisson kernel $\fun{Q_{h}}{\mathbf{\eta} \: \cdot \,}$ is a normalised positive function with
$\|\fun{Q_{h}}{\mathbf{\eta} \: \cdot \,}\|_{\Ln{1}{\mathbb{S}^2}}=1$.
Further properties of the Poisson kernel $\fun{Q_{h}}{\mathbf{\eta} \: \cdot \,}$ and the 
singularity kernel $\fun{S_{h}}{\mathbf{\eta} \: \cdot \,}$
with respect to localisation and smoothness are derived
in \cite[pp. 112]{frgesc}.

\begin{figure}[tb]
  \centering
  \subfigure[$\fun{Q_{h}}{\cos\vartheta}$, $h=0.5,0.7,0.8$.]
  {\includegraphics[width=0.45\textwidth]{images/poisson}}\hfill
  \subfigure[$\fun{S_{h}}{\cos\vartheta}$, $h=0.8,0.9,0.955$.]
  {\includegraphics[width=0.45\textwidth]{images/singularity}}
  \caption{The kernels $\fun{Q_{h}}{\cos\vartheta}$ and $\fun{S_{h}}{\cos\vartheta}$
  for different values of $h$.}
  \label{Basics:Figure:PoissonSingularityKernel}
\end{figure}

For both kernels the Fourier-Legendre coefficients $\fun{K^{\wedge}}{k}$ are
explicitly known, such that we simply state the following lemma:

\begin{lemma}
  ${}^{}$\\[-2ex]
 \begin{enumerate}
   \item 
Using the Poisson kernel $K=Q_h$, cf. Definition \ref{def:poisson_sing} 1.,
in our summation algorithm yields a relative error of
     \begin{equation}
       \label{error:poisson}
       \frac{\left\|f - f_{M}\right\|_{\infty}}{\left\|\mathbf{b}\right\|_1} \le
       \frac{h^{M+1}}{4\pi}
       \left(\frac{2M+1}{1-h}+\frac{2}{\left(1-h\right)^2}\right)\, .
     \end{equation}
     \item 
Using the singularity kernel $K=S_h$, cf. Definition \ref{def:poisson_sing}
2., in our summation algorithm yields a relative error of 
       \begin{equation}
         \label{error:singular}
         \frac{\left\|f - f_{M}\right\|_{\infty}}{\left\|\mathbf{b}\right\|_1} \le
         \frac{h^{M+1}}{4\pi} \left(\frac{2M+1}{2\left(1-h\right)}+
           \frac{4M}{\left(1-h\right)^2}+
         \frac{4}{\left(1-h\right)^3}\right)\, .
       \end{equation}
 \end{enumerate}
\end{lemma}
\begin{proof}
The Fourier-Legendre coefficients are given by $\fun{Q_{h}^{\wedge}}{k}=h^k$ and
$\fun{S_{h}^{\wedge}}{k}=\frac{2}{2k+1}h^k$, respectively (see \cite[pp. 107]{frgesc}).
Using Lemma \ref{lemma:error} yields the assertions.
\end{proof}
  
Simply put, our scheme achieves (almost) accuracy $\epsilon$ for $M \ge \log\epsilon \,
/ \, \log h$.

\subsection{Locally supported kernel}
\begin{definition}
  Let $h \in (-1,1)$ and $\lambda \in \NZ$.
  The locally supported kernel
    $L_{h,\lambda}:[-1,1] \rightarrow \R$, considered in
    \cite{Sc97}, is defined by
    \[
    \fun{L_{h,\lambda}}{x} := 
    \left\{\begin{array}{l@{\quad \text{if} \quad}rcl} 
        0 & -1 &\le x \le& h, \\
        \frac{\lambda+1}{2\pi(1-h)^{\lambda+1}}\left(x-h\right)^{\lambda} &  h & <  x \le& 1.
      \end{array}\right.
    \]
\end{definition}

Figure \ref{Basics:Figure:LKernel} shows the function $L_{h,\lambda}$ for
different values $h$ and $\lambda$.
\begin{figure}[tb]
  \centering
  \subfigure[$\lambda=1.0$]
  {\includegraphics[width=0.45\textwidth]{images/locsup1}}\hfill
  \subfigure[$\lambda=2.0$]
  {\includegraphics[width=0.45\textwidth]{images/locsup2}}
  \caption{The locally supported kernel $\fun{L_{h,\lambda}}{\cos\vartheta}$ 
  for $h = -0.7, 0.2, 0.7$ and different values of $\lambda$.}
  \label{Basics:Figure:LKernel}
\end{figure}
While the parameter $h$ again steers the localisation in spatial domain, the
parameter $\lambda$ corresponds to the smoothness of $L_{h,\lambda}$ in the
endpoint of the support $h$.
We have the following lemma:

\begin{lemma}
  For the locally supported kernel $L_{h,\lambda}$ holds:
  \begin{enumerate}
  \item The Fourier-Legendre coefficients $\fun{L_{h,\lambda}^{\wedge}}{k}$ can be computed recursively by
    \[
    \fun{L_{h,\lambda}^{\wedge}}{k+1} = \frac{\left(2k+1\right) h}{k+\lambda+2}
    \fun{L_{h,\lambda}^{\wedge}}{k}   - \frac{k-\lambda-1}{k+\lambda+2}
    \fun{L_{h,\lambda}^{\wedge}}{k-1}
    \]
    for $k\in \N$ where $\fun{L_{h,\lambda}^{\wedge}}{0} = 1$ and
    $\fun{L_{h,\lambda}^{\wedge}}{1} = \frac{\lambda + 1 + h}{\lambda+2}$.
  \item We obtain for $\lambda \in \NZ$ and $k>\lambda+1$ the decay rate
    \[
    \left|\fun{L_{h,\lambda}^{\wedge}}{k}\right| \le
    \frac{2\sqrt{2}}{\left(2k+1\right)\sqrt{\pi}}\;
    \frac{\left(\lambda+1\right)^2}{\left(1-h\right)^{2\lambda+1}\sqrt[4]{1-|h|}}\;
    \frac{1}{\left(k-\lambda\right)^{\lambda+\frac{1}{2}}}\,. 
    \]
  \item Thus, the relative error of our summation algorithm with this kernel
  $K=L_{h,\lambda}$ is bounded for $\lambda \in \N$ and $M>\lambda$ by
  \begin{equation}
    \label{error:Lh}
    \frac{\left\|f - f_{M}\right\|_{\infty}}{\left\|\mathbf{b}\right\|_1} \le
    \frac{1}{\pi\sqrt{2\pi}}\;
    \frac{\left(\lambda+1\right)^2}{\lambda-\frac{1}{2}}\;
    \frac{1}{\left(1-h\right)^{2\lambda+1}\sqrt[4]{1-|h|}}\;
    \frac{1}{\left(M-\lambda\right)^{\lambda-\frac{1}{2}}}\,.
  \end{equation}
  \end{enumerate}
\end{lemma}
\begin{proof}
  ${}^{}$\\[-2ex]
  \begin{enumerate}
  \item We apply in \eqref{equation:legtrafo} the recurrence relations \eqref{three1} and \eqref{three2}.
  \item For $\lambda=0$, we have
    \begin{equation*}
      \left|\fun{L_{h,\lambda}^{\wedge}}{k}\right| 
      = \left| 2\pi \int_{h}^1 \frac{\lambda+1}{2\pi\left(1-h\right)^{\lambda+1}}
        \left(x-h\right)^{\lambda} \fun{P_{k}}{x} \: \dx x\right| 
      \le \frac{\lambda+1}{(1-h)^{\lambda+1}}\left|\int_{h}^1 \fun{P_{k}}{x} \: \dx x\right|.
    \end{equation*}
    Using \eqref{three2} and \eqref{leg_prop}, we obtain
    \begin{align*}
      \left|\int_{h}^1 \fun{P_{k}}{x} \: \dx x\right|
      %& = \frac{1}{2k+1}
      %    \abs{\int_{h}^1 \fun{P_{k+1}'}{x} - \fun{P_{k-1}'}{x} \: \dx x}\\
      & = \frac{1}{2k+1}
          \left|\fun{P_{k-1}}{h} - \fun{P_{k+1}}{h}\right|\\
      & \le \frac{1}{2k+1} \sqrt{\frac{2}{\pi \sin\arccos h}} \left(\frac{1}{\sqrt{k-1}} + \frac{1}{\sqrt{k+1}}\right),
    \end{align*}
    and with $\sin\arccos h = \sqrt{1-h^2} \ge \sqrt{1-|h|}$ and
    $\frac{1}{\sqrt{k-1}} + \frac{1}{\sqrt{k+1}} \le
    \frac{2k+1}{k\sqrt{k}}$ finally
    \begin{equation}
      \label{SmallLambda}
      \left|\fun{L_{h,0}^{\wedge}}{k}\right| \le 
      \frac{\sqrt{2}}{\sqrt{\pi}}\;
      \frac{\lambda+1}{(1-h)^{\lambda+1}\sqrt[4]{1-|h|}}\;
      \frac{1}{k\sqrt{k}}.
    \end{equation}
%    \frac{1}{1-h}
%    \left|\int_{h}^{1} \fun{P_{k}}{x} \dx x\right| \le
%    \frac{\sqrt{2}}{\left(2k+1\right)\left(1-h\right)\sqrt{\pi}\sqrt[4]{1-h^2}}
%    \left(\frac{1}{\sqrt{k-1}}+\frac{1}{\sqrt{k+1}}\right)\,.
    Furthermore, for $\lambda \in \N$, we obtain by applying \eqref{three2} that
    \begin{equation*}
      \left|\fun{L_{h,\lambda}^{\wedge}}{k}\right| 
      %&\le&
      %\frac{\lambda+1}{\lambda\left(1-h\right)\left(2k+1\right)}
      %\left|\fun{L_{h,\lambda-1}^{\wedge}}{k-1}-
      %  \fun{L_{h,\lambda-1}^{\wedge}}{k+1}\right|\\ &\le&
      \le \frac{2}{2k+1} \frac{\lambda+1}{\lambda\left(1-h\right)}
      \max\limits_{|k-k'|\le 1}
      \left|\fun{L_{h,\lambda-1}^{\wedge}}{k'}\right|.
    \end{equation*}
    Iterate this argument and estimate $\frac{2}{2k'+1}\le
    \frac{1}{k-\lambda}$ yields
    \begin{align*}
      \left|\fun{L_{h,\lambda}^{\wedge}}{k}\right|
      & \le \frac{2}{2k+1} \frac{\lambda+1}{\left(1-h\right)^{\lambda}}
        \frac{1}{\left(k-\lambda\right)^{\lambda-1}} 
        \max_{|k-k'| \le \lambda} 
        \left|\fun{L_{h,0}^{\wedge}}{k'}\right|.
    \end{align*}
    We finally use \eqref{SmallLambda} for $k'=k-\lambda$ to obtain the
    assertion.
  \item We combine 2. with Lemma \ref{lemma:error}. 
  \end{enumerate}
\end{proof}

Simply put, our scheme achieves accuracy of order $\epsilon$ for $M \ge
\lambda + (1-h)^{-2} {\epsilon}^{-1/\lambda}$.

\subsection{Spherical Gaussian kernel}
The spherical analogue to the well-known \emph{Gaussian kernel} $\e^{-\sigma x^2}$ is the 
\emph{spherical Gaussian kernel}, see for example \cite{bahu01}.
\begin{definition}
  For $\sigma>0$, the spherical Gaussian kernel
  $G_{\sigma}:[-1,1] \rightarrow \R$ is given by
  \begin{equation}
    \label{GaussKernel}
    \nonumber
    \fun{G_{\sigma}}{x} := \e^{2\sigma x-2\sigma}\,.
  \end{equation}
\end{definition}

Figure \ref{Basics:Figure:GKernel} shows the spherical Gaussian kernel $G_{\sigma}$ for
different values $\sigma$.
\begin{figure}[tb]
  \centering
  \includegraphics[width=0.45\textwidth]{images/gaussian}
  \caption{The $\text{L}^2$-normalised spherical Gaussian kernel 
  $\fun{\gamma}{\sigma} \fun{G_{\sigma}}{\cos\vartheta}$ with
  $\fun{\gamma}{\sigma} := \left(\frac{2\sigma}{\pi}\right)^{1/2} \left(1-\e^{-8\sigma}\right)^{-1/2}$ for $\sigma = 1,5,20$.}
  \label{Basics:Figure:GKernel}
\end{figure}

\newpage

\begin{lemma}
\label{lem:gauss}
For the spherical Gaussian $G_{\sigma}$ holds:
  \begin{enumerate}
  \item The Fourier-Legendre coefficients $\fun{G_{\sigma}^{\wedge}}{k}$
    are given by
    \begin{equation}
      \label{gaussian:explicit}
      \fun{G_{\sigma}^{\wedge}}{k} = 2 \sigma^{-\frac{1}{2}} \e^{-2\sigma}
      \pi^{\frac{3}{2}} I_{k+\frac{1}{2}}\left(2\sigma\right)
    \end{equation}
    where $I_{k+\frac{1}{2}}$ denotes the modified Bessel function of first
    kind, see \cite{abst}.
                                %Furthermore, $\fun{G_{\sigma}^{\wedge}}{k} \ge 0$.
  \item Thus, the relative error of our summation algorithm with the spherical Gaussian
    kernel $K=G_{\sigma}$ is bounded by
    \begin{equation}
      \label{error:G}
      \frac{\left\|f - f_{M}\right\|_{\infty}}{\left\|\mathbf{b}\right\|_1} \le
      \frac{\sqrt{\pi\sigma}\left(\e^{\sigma}-1\right) \sigma^{M-\frac{1}{2}}}{\fun{\Gamma}{M
          +\frac{1}{2}}}.
    \end{equation} 
\end{enumerate}
\end{lemma}
\begin{proof}
  ${}^{}$\\[-2ex]
  \begin{enumerate}
  \item The relation \eqref{three2} and integration by parts yields the
    difference equation
    \begin{equation}\label{eq:gauss_rec}
    {2\sigma}\fun{G_{\sigma}^{\wedge}}{k-1} - {2\sigma}\fun{G_{\sigma}^{\wedge}}{k+1} = 
    \left(2k+1\right)\fun{G_{\sigma}^{\wedge}}{k}
    \end{equation}
    for $k\in \N$ where $\fun{G_{\sigma}^{\wedge}}{0} = 4 \pi \sigma^{-1}
    \e^{-2\sigma} \sinh \sigma \cosh \sigma$ and
    $\fun{G_{\sigma}^{\wedge}}{1} = \pi \sigma^{-2} \e^{-2\sigma} (2 \sigma
    \cosh 2 \sigma + \sinh \sigma )$.
    The assertion follows than e.g. by \cite{bahu01} from the
    definition of the midified Bessel function. 

{\bf Oder sollten wir statt auf Huber nochmals auf [12]
    verweisen. Bei Hubber steht auch nicht mehr}

  \item Using 1. and
  \begin{eqnarray*}
    \frac{2k+1}{4\pi} \left|\fun{G_{\sigma}^{\wedge}}{k}\right|
    &=&
    \frac{\e^{-2\sigma}
    \left(k+\frac{1}{2}\right)\sigma^k}{\fun{\Gamma}{k+1}} \int_{-1}^{1}
    \e^{2\sigma x} \left(1-x^2\right)^k \dx x\\
    &\le&
    \frac{\left(k+\frac{1}{2}\right)\sigma^k}{\fun{\Gamma}{k+1}}
    \int_{-1}^{1} \left(1-x^2\right)^k \dx x\\
    &=&
    \frac{\sqrt{\pi}\sigma^k}{\fun{\Gamma}{k+\frac{1}{2}}}
  \end{eqnarray*}
  in conjunction with
  \begin{equation*}
    \sum_{k>M} \frac{\sqrt{\pi}\sigma^k}{\fun{\Gamma}{k+\frac{1}{2}}}\\
    =
    \frac{\sqrt{\pi\sigma}\e^{\sigma}}{\fun{\Gamma}{M+\frac{1}{2}}} 
    \int_{0}^{\sigma} t^{M-\frac{1}{2}} \e^{-t} \dx t\\
    \le
    \frac{\sqrt{\pi\sigma}\left(\e^{\sigma}-1\right)
    \sigma^{M-\frac{1}{2}}}{\fun{\Gamma}{M+\frac{1}{2}}}
  \end{equation*}
  yields the assertion.
\end{enumerate}
\end{proof}

Note that using the difference equation \eqref{eq:gauss_rec} in a forward
recursion is numerically unstable and we therefore use routines for
evaluating Bessel functions provided by
the \emph{GNU scientific library (GSL)} \cite{gsl} in the
pre-computation of the Fourier-Legendre coefficients $\fun{G^{\wedge}}{k}$ 
by means of \eqref{gaussian:explicit}.

We conclude this section with the following corollary on the matrix norm
approximation.
\begin{corollary}
  \label{cor:rankapprox}
  Let $L \in \N$, a set of arbitrary source nodes $\mathcal{Y} =
  \left\{\mathbf{\eta}_{l} \in \mathbb{S}^2:\ l = 0,\ldots,L-1\right\}$, and a set of
  arbitrary target nodes $\mathcal{X} := \left\{\mathbf{\xi}_{l} \in
  \mathbb{S}^2:\ l=0,\ldots,L-1\right\}$ be given.
  Then, the proposed approximation $\mathbf{Y_{\mathcal{X}}} \: \mathbf{\hat W} \:
  \mathbf{Y_{\mathcal{Y}}}^{\adj}$ to the matrix $\mathbf{K}\in
  \R^{L\times L}$, cf. Remark
  \ref{rem:rankM}, fulfils
  \begin{equation*}
    \left\| \mathbf{K} - \mathbf{Y_{\mathcal{X}}} \: \mathbf{\hat W} \:
      \mathbf{Y_{\mathcal{Y}}}^{\adj} \right\|_p
    \le L \: C_M
  \end{equation*}
  for $1\le p \le \infty$ where $C_M$ denotes the right hand sides in one of
  the inequalities \eqref{error:poisson}, \eqref{error:singular},
  \eqref{error:Lh}, or \eqref{error:G}. 
\end{corollary}
\begin{proof}
 Using Hoelder's inequality, we obtain
 \[
 \left\| \mathbf{K} - \mathbf{Y_{\mathcal{X}}} \: \mathbf{\hat W} \:
   \mathbf{Y_{\mathcal{Y}}}^{\adj} \right\|_p
 = \max_{\mathbf{b}\in \R^L} \frac{\left\|\mathbf{f} -
     \mathbf{f}_{M}\right\|_{p}}{\left\|\mathbf{b}\right\|_p}
 \le \max_{\mathbf{b}\in \R^L} L^{\frac{1}{p}} L^{1-\frac{1}{p}} \frac{\left\|\mathbf{f}
     - \mathbf{f}_{M}\right\|_{\infty}}{\left\|\mathbf{b}\right\|_1}\,.
 \]
 The assertion follows by $\|\mathbf{f} - \mathbf{f}_{M}\|_{\infty} \le
  \|f-f_M\|_{\infty}$ and the estimates
  \eqref{error:poisson}, \eqref{error:singular}, \eqref{error:Lh}, or
  \eqref{error:G}.
\end{proof}

Using for example the Poisson kernel, we achieve an approximation of the square matrix
$\mathbf{K}\in\R^{L \times L}$ up to an prescribed accuracy $\varepsilon$ by
choosing the truncation parameter $M\ge \log(\frac{\varepsilon}{L}) /\log h$.
Thus, for the Poisson kernel, we have to increase the cut-off degree $M$ by
$\log\frac{1}{2} / \log h$ whenever we double the number $L$ of nodes, to achieve
the same accuracy.

%=============================================================================
\section{Numerical results}
%=============================================================================
We present numerical examples in order to demonstrate the performance of
our approach. All algorithms were implemented in C and tested on an 
AMD Athlon\texttrademark XP 2700+ with 2GB main memory, SuSE-Linux 
(kernel 2.4.20-4GB-athlon, gcc 3.3) using double precision arithmetic. 
Moreover, we have used the libraries FFTW 3.0.1 \cite{fftw}, NFFT 2
\cite{kupo02C} and a custom NFSFT library which will be part of the next 
major release of the NFFT library. Throughout our experiments we have 
applied the NFFT package \cite{kupo02C} with pre-computed Kaiser--Bessel 
functions and an oversampling factor $\rho=2$.

In our tests we have always chosen uniformly distributed pseudo-random 
source and target nodes 
$\left(\vartheta,\varphi\right) \in [0,\pi] \times [-\pi,\pi)$ and 
coefficients $b_l$ from $\left[-\frac{1}{2},\frac{1}{2}\right]$.

We have considered the Poisson kernel $Q_{h}$,
the singularity kernel $S_{h}$,
the locally supported kernel $L_{h,\lambda}$,
and the Gaussian kernel $G_{\sigma}$.

\begin{example}
First, we examine the systematic error due to our approximation
\eqref{Applications:TruncatedSeries} and the use of the approximative NFSFT
algorithms. Figures \ref{fig:error} and \ref{fig:gtest2} show the error
\[
E_{\infty}:=
\frac{\left\|\mathbf{f}-\mathbf{f}_M\right\|_{\infty}}{\left\|\mathbf{b}\right\|_{1}}
\quad \approx \quad \frac{\left\|f-f_M\right\|_{\infty}}{\left\|\mathbf{b}\right\|_{1}}
\]
for the mentioned kernels as a function of the cut-off degree $M$.
Here the vector $\mathbf{f}$ contains the values $f(\xi_d)$ for
$d=0,\ldots, 999$. We compute the vector $\mathbf{f}_M$ by
\eqref{Eq:truncf_M} by the straightforward summation (NDSFT) and by
Algorithm  \ref{Applications:Algorithm:FastSummation} where we use the NFSFT.
Furthermore we plot the error estimates given in \eqref{error:poisson},
\eqref{error:singular}, \eqref{error:Lh}, and \eqref{error:G}.
\end{example}

\begin{figure}[tb]
  \centering
  \subfigure[The Poisson kernel $Q_{h}$ for $h = 0.8$.]
  {\includegraphics[width=0.45\textwidth]{images/poisson_test}}\hfill
  \subfigure[The Singularity kernel $S_{h}$ for $h = 0.8$.]
  {\includegraphics[width=0.45\textwidth]{images/singularity_test}}\\
  \subfigure[The locally supported kernel $L_{h,\lambda}$ for $h=0.3$ and $\lambda = 7$.]
  {\includegraphics[width=0.45\textwidth]{images/locsupp_test}}\hfill
  \subfigure[The spherical Gaussian kernel $G_{\sigma}$ for $\sigma=100$. The 
  error estimate from Lemma \ref{lem:gauss} does not provide a useful bound
  for this value of $\sigma$ and is not shown. Compare with Figure \ref{fig:gtest2}.]
  {\includegraphics[width=0.45\textwidth]{images/gaussian_test}}
  \caption{The error $E_{\infty}$ for $M = 4,8,\ldots,256$ and $L = D = 1000$: 
  Fast summation with NDSFT (solid), fast summation with NFSFT and 
  NFFT cut-off parameter $m = 3$ (dash-dot), fast summation with NFSFT and NFFT cut-off 
  parameter $m = 6$ (dashed), error estimate for 
  $E_{\infty}$ (dotted).}
  \label{fig:error}
\end{figure}

\begin{figure}[tb]
  \centering
%  \subfigure[To be written...]
%  {
  \includegraphics[width=0.45\textwidth]{images/gaussian_test2}
%  }\hfill
%  \subfigure[The Poisson kernel $Q_{h}$ for $h = 0.8$: Fast summation with 
%  NFSFT/NDSFT and thresholds $10^6$ (solid), $10^9$ (dashed), $10^9$ (dash-dot). 
%  The threshold steers the accuracy of the FLFT transformation.]
%  {
%    \includegraphics[width=0.45\textwidth]{images/threshold_test}
%  }\hfill
  \caption{The error $E_{\infty}$ for $M = 1,2,\ldots,32$ and $L = D = 1000$: 
  Fast summation with NDSFT (solid), fast summation with NFSFT and 
  NFFT cut-off parameter $m = 3$ (dash-dot), fast summation with NFSFT and NFFT cut-off 
  parameter $m = 6$ (dashed), error estimate for $E_{\infty}$ (dotted). We used the
  spherical Gaussian kernel $G_{\sigma}$ with small $\sigma=5$.}
  \label{fig:gtest2}
\end{figure}

{\bf Figure 5.2 in Figure 5.1 d setzen}

\begin{example}
We now compare the computation time of the straightforward summation
(direct alg.), the 
straightforward summation with pre-computed matrix $\mathbf{K}$
(w/pre-comp.) , the fast summation 
algorithm with NDSFT (FS, NDSFT), and the fast summation algorithm
with NFSFT (FS, NFSFT) for 
increasing $D=L$ and fixed cut-off degree $M=128$. 

{\bf was ist error  ( Fehler zwischen direct alg and FS, NFSFT oder NDSFT?} 

The CPU time required by the four algorithms is shown in Table
\ref{tab:TimeSpace}. 
As expected, the fast NDSFT and NFSFT summation algorithms outperform the 
straightforward algorithms, but with the NFSFT--version considerably
faster. 
\end{example}

\begin{table}[ht!]
  \begin{center}
    \begin{tabular}{r|r|r|r|r|r}
         $L = D$ &      direct alg.   &     w/pre-comp.     &      FS, NDSFT &      FS, NFSFT & error $E_{\infty}$\\\hline\\[-2.0ex]
           $2^6$ & \verb#1.0E-05#     & \verb#8.0E-05#     & \verb#1.1E-01# & \verb#6.2E-01# & \verb#7.7E-14# \\
           $2^7$ & \verb#6.0E-05#     & \verb#3.8E-04#     & \verb#2.2E-01# & \verb#6.2E-01# & \verb#6.5E-14# \\
           $2^8$ & \verb#2.5E-04#     & \verb#1.4E-03#     & \verb#4.5E-01# & \verb#6.2E-01# & \verb#4.1E-14# \\
           $2^9$ & \verb#1.0E-03#     & \verb#5.3E-03#     & \verb#8.9E-01# & \verb#6.3E-01# & \verb#2.8E-14# \\
        $2^{10}$ & \verb#4.0E-02#     & \verb#2.1E-02#     & \verb#1.8E+00# & \verb#6.5E-01# & \verb#3.6E-14# \\
        $2^{11}$ & \verb#1.6E+00#     & \verb#8.3E-02#     & \verb#3.6E+00# & \verb#6.6E-01# & \verb#1.8E-14# \\
        $2^{12}$ & \verb#6.4E+00#     & \verb#3.5E-01#     & \verb#7.1E+00# & \verb#7.2E-01# & \verb#1.3E-14# \\
        $2^{13}$ & \verb#2.6E+01#     & \verb#1.4E+00#     & \verb#1.4E+01# & \verb#8.2E-01# & \verb#6.7E-15# \\
        $2^{14}$ & \verb#1.0E+02#     & $^*$\verb#5.6E+00# & \verb#2.8E+01# & \verb#1.0E+00# & \verb#5.5E-15# \\
        $2^{15}$ & \verb#4.1E+02#     & $^*$\verb#2.2E+01# & \verb#5.7E+01# & \verb#1.5E+00# & \verb#4.0E-15# \\
        $2^{16}$ & \verb#1.6E+03#     & $^*$\verb#8.9E+01# & \verb#1.1E+02# & \verb#2.3E+00# & \verb#2.9E-15# \\
        $2^{17}$ & \verb#6.6E+03#     & $^*$\verb#3.6E+02# & \verb#2.3E+02# & \verb#4.0E+00# & \verb#2.4E-15# \\
        $2^{18}$ & \verb#2.6E+04#     & $^*$\verb#1.4E+03# & \verb#4.6E+02# & \verb#7.5E+00# & \verb#1.9E-15# \\
        $2^{19}$ & $^*$\verb#1.0E+05# & $^*$\verb#5.7E+03# & \verb#9.1E+02# & \verb#1.4E+01# & \verb#-# \\
        $2^{20}$ & $^*$\verb#4.2E+05# & $^*$\verb#2.3E+04# & \verb#1.8E+03# & \verb#2.8E+01# & \verb#-# \\
        $2^{21}$ & $^*$\verb#1.7E+06# & $^*$\verb#9.1E+04# & \verb#3.6E+03# & \verb#5.5E+01# & \verb#-# \\
    \end{tabular}

  \end{center}
  \caption{CPU-Time and error $E_{\infty}$ for the fast summation algorithm with cut-off degree $M = 128$.
    Note that we used accumulated measurements in case of small times. Values 
    marked with $^{*}$ were extrapolated owing to CPU-time and memory limitations.}
  \label{tab:TimeSpace}
\end{table}

\begin{example}
As an example for the result provided by Corollary \ref{cor:rankapprox}, we 
finally consider the Poisson kernel $Q_{h}$ for 
$h = 0.1$ and $h=0.5$, $L=D=400$ arbitrary source and target nodes $\mathbf{\eta}_{l}$ and $\mathbf{\xi}_{d}$,
and our particular rank $(M+1)^2$ approximation 
$\mathbf{Y_{\mathcal{X}}} \: \mathbf{\hat W} \: \mathbf{Y_{\mathcal{Y}}}^{\adj}$ to the matrix $\mathbf{K} \in \R^{L \times L}$ 
for cut-off degrees $M=0,\ldots,19$. Note that for $M=19$ we 
have $(M+1)^2 = 400 = L = D$ and therefore $\mathbf{Y_{\mathcal{X}}}, \mathbf{\hat W}, 
\mathbf{Y_{\mathcal{Y}}}^{\adj} \in \C^{L \times L}$ which renders our approximation a full-rank 
approximation. We compare the performance of our approach $\mathbf{Y_{\mathcal{X}}} \: \mathbf{\hat W} \: 
\mathbf{Y_{\mathcal{Y}}}^{\adj}$ and the corresponding error estimate from Corollary \ref{cor:rankapprox}
with the truncated-SVD rank $l$ approximation $\mathbf{S}_{l}$ for $l = 1,\ldots,L$
measured in the spectral norm $\left\|\cdot\right\|_{2}$. The results are shown in Figure \ref{fig:rankapprox}. 

{\bf und zeigen ...}
 
\end{example}

\begin{figure}[tb]
  \centering
  \subfigure[$h=0.1$]
  {\includegraphics[width=0.45\textwidth]{images/poisson_h_0_1}}\hfill
  \subfigure[$h=0.5$]
  {\includegraphics[width=0.45\textwidth]{images/poisson_h_0_5}}\\
  \caption{Comparison of our approximation $\mathbf{Y_{\mathcal{X}}} \: \mathbf{\hat W} \: 
  \mathbf{Y_{\mathcal{Y}}}^{\adj}$ to the matrix $\mathbf{K}$ with the truncated-SVD approximation
  for the Poisson kernel $Q_{h}$ with $h=0.1$ (left) and $h=0.5$ (right), 
  and $L = D = 400$ arbitrary source and target nodes. 
  We compare 
  $\|\mathbf{K} - \mathbf{Y_{\mathcal{X}}} \: \mathbf{\hat W} \: \mathbf{Y_{\mathcal{Y}}}^{\adj}\|_{2}$ 
  ($+$) and the error estimate $L \: C_{M}$ ($\times$) from Corollary \ref{cor:rankapprox} 
  for cut-off degree $M=0,\ldots,19$ with 
  $\left\|\mathbf{K} - \mathbf{S}_{l}\right\|_{2}$ (solid) for 
  the truncated-SVD rank $l$ approximation $\mathbf{S}_{l}$ for $l=1,\ldots,L$.
  }
  \label{fig:rankapprox}
\end{figure}

%=============================================================================
\section{Conclusions}
%=============================================================================
We presented a fast algorithm for the computation of sums of type
\eqref{Applications:KernelSum} in ${\cal O} (D + L)$ arithmetic operations.
For a range of zonal functions, we established error estimates 
concerning the dependence of the computational speed on the desired accuracy 
and the function's parameters. The numerical results confirm the theoretical 
expectations.
The software for this algorithm including all described tests can be
obtained from the authors.

%=============================================================================
\bibliographystyle{abbrv}
\bibliography{../references}
\end{document}
